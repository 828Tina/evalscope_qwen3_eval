{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2c85da4",
   "metadata": {},
   "source": [
    "# 训练代码\n",
    "\n",
    "1. 保存模型和数据集\n",
    "2. 处理数据集\n",
    "3. 开启ms-swift训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcebf88b",
   "metadata": {},
   "source": [
    "---\n",
    "1. 保存模型和数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2731d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Dataset to directory: /data/nvme0/zh_cot_110k_sft\n",
      "Processing 4 items:   0%|                                 | 0/4 [00:00<?, ?it/s]\n",
      "Downloading [dataset_infos.json]:   0%|               | 0.00/419 [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "Downloading [.gitattributes]:   0%|                 | 0.00/3.73k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:   0%|       | 0.00/647M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading [README.md]:   0%|                      | 0.00/4.86k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading [dataset_infos.json]: 100%|██████████| 419/419 [00:00<00:00, 774B/s]\u001b[A\n",
      "Processing 4 items:  25%|██████▎                  | 1/4 [00:00<00:01,  1.84it/s]\n",
      "\n",
      "\n",
      "\n",
      "Downloading [README.md]: 100%|█████████████| 4.86k/4.86k [00:00<00:00, 8.24kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [.gitattributes]: 100%|████████| 3.73k/3.73k [00:00<00:00, 6.17kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:   0%| | 1.00M/647M [00:00<08:33, 1.32MB\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:   1%| | 4.00M/647M [00:00<01:55, 5.84MB\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:   1%| | 7.00M/647M [00:01<01:05, 10.3MB\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:   2%| | 11.0M/647M [00:01<00:42, 15.7MB\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:   2%| | 14.0M/647M [00:01<00:37, 17.6MB\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:   3%| | 17.0M/647M [00:01<00:50, 13.1MB\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:   3%| | 20.0M/647M [00:01<00:43, 15.2MB\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:   4%| | 23.0M/647M [00:01<00:36, 18.1MB\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:   4%| | 26.0M/647M [00:02<00:35, 18.4MB\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:   4%| | 29.0M/647M [00:02<00:31, 20.3MB\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:   5%| | 32.0M/647M [00:02<00:30, 21.2MB\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:   5%| | 35.0M/647M [00:02<00:31, 20.3MB\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:   6%| | 38.0M/647M [00:02<00:29, 21.8MB\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:   6%| | 42.0M/647M [00:02<00:33, 19.2MB\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:   7%| | 45.0M/647M [00:02<00:29, 21.1MB\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:   7%| | 48.0M/647M [00:03<00:28, 22.3MB\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:   8%| | 51.0M/647M [00:03<00:26, 23.9MB\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:   9%| | 55.0M/647M [00:03<00:23, 26.6MB\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:   9%| | 59.0M/647M [00:03<00:21, 28.5MB\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  10%| | 62.0M/647M [00:03<00:21, 28.3MB\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  10%| | 65.0M/647M [00:03<00:23, 26.0MB\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  11%| | 69.0M/647M [00:03<00:21, 27.8MB\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  11%| | 72.0M/647M [00:03<00:21, 27.5MB\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  12%| | 75.0M/647M [00:04<00:21, 28.2MB\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  12%| | 78.0M/647M [00:04<00:21, 28.4MB\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  13%|▏| 82.0M/647M [00:04<00:20, 29.2MB\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  13%|▏| 85.0M/647M [00:04<00:20, 28.1MB\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  14%|▏| 89.0M/647M [00:04<00:20, 28.5MB\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  14%|▏| 92.0M/647M [00:04<00:20, 28.8MB\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  15%|▏| 95.0M/647M [00:04<00:19, 29.2MB\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  15%|▏| 98.0M/647M [00:04<00:20, 28.0MB\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  16%|▏| 102M/647M [00:05<00:19, 29.4MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  16%|▏| 105M/647M [00:05<00:19, 28.9MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  17%|▏| 108M/647M [00:05<00:19, 28.8MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  17%|▏| 111M/647M [00:05<00:21, 26.7MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  18%|▏| 114M/647M [00:05<00:20, 27.7MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  18%|▏| 117M/647M [00:05<00:19, 27.8MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  19%|▏| 120M/647M [00:05<00:22, 24.1MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  19%|▏| 123M/647M [00:05<00:21, 25.6MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  19%|▏| 126M/647M [00:05<00:20, 26.3MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  20%|▏| 129M/647M [00:06<00:20, 27.1MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  20%|▏| 132M/647M [00:06<00:19, 27.7MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  21%|▏| 135M/647M [00:06<00:21, 25.0MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  21%|▏| 138M/647M [00:06<00:23, 22.5MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  22%|▏| 141M/647M [00:06<00:22, 23.7MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  22%|▏| 144M/647M [00:06<00:20, 25.5MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  23%|▏| 148M/647M [00:06<00:18, 27.7MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  23%|▏| 152M/647M [00:07<00:17, 29.1MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  24%|▏| 155M/647M [00:07<00:20, 25.6MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  24%|▏| 158M/647M [00:07<00:21, 24.1MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  25%|▎| 162M/647M [00:07<00:19, 26.6MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  26%|▎| 165M/647M [00:07<00:22, 22.7MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  26%|▎| 169M/647M [00:07<00:19, 25.2MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  27%|▎| 172M/647M [00:07<00:18, 26.5MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  27%|▎| 175M/647M [00:07<00:18, 27.0MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  28%|▎| 178M/647M [00:08<00:17, 27.9MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  28%|▎| 181M/647M [00:08<00:17, 27.3MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  28%|▎| 184M/647M [00:08<00:19, 25.2MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  29%|▎| 187M/647M [00:08<00:18, 26.4MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  29%|▎| 190M/647M [00:08<00:19, 24.9MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  30%|▎| 193M/647M [00:08<00:19, 24.5MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  30%|▎| 196M/647M [00:08<00:18, 25.5MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  31%|▎| 199M/647M [00:08<00:17, 26.5MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  31%|▎| 202M/647M [00:09<00:16, 27.6MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  32%|▎| 205M/647M [00:09<00:16, 27.9MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  32%|▎| 208M/647M [00:09<00:16, 27.6MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  33%|▎| 211M/647M [00:09<00:16, 27.9MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  33%|▎| 214M/647M [00:09<00:16, 28.3MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  34%|▎| 217M/647M [00:09<00:15, 28.4MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  34%|▎| 220M/647M [00:09<00:16, 27.3MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  34%|▎| 223M/647M [00:09<00:16, 26.9MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  35%|▎| 226M/647M [00:09<00:15, 28.0MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  35%|▎| 229M/647M [00:10<00:18, 23.4MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  36%|▎| 233M/647M [00:10<00:16, 25.9MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  36%|▎| 236M/647M [00:10<00:16, 25.8MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  37%|▎| 240M/647M [00:10<00:15, 27.8MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  38%|▍| 243M/647M [00:10<00:15, 27.9MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  38%|▍| 246M/647M [00:10<00:16, 26.2MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  38%|▍| 249M/647M [00:10<00:15, 26.5MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  39%|▍| 252M/647M [00:11<00:17, 24.1MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  39%|▍| 255M/647M [00:11<00:16, 25.5MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  40%|▍| 258M/647M [00:11<00:16, 25.3MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  40%|▍| 261M/647M [00:11<00:16, 25.1MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  41%|▍| 264M/647M [00:11<00:16, 24.2MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  41%|▍| 267M/647M [00:11<00:16, 24.3MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  42%|▍| 270M/647M [00:11<00:16, 24.4MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  42%|▍| 273M/647M [00:11<00:16, 24.4MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  43%|▍| 276M/647M [00:12<00:15, 24.5MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  43%|▍| 279M/647M [00:12<00:16, 23.8MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  44%|▍| 282M/647M [00:12<00:16, 23.6MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  44%|▍| 285M/647M [00:12<00:15, 24.5MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  45%|▍| 288M/647M [00:12<00:16, 23.3MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  45%|▍| 291M/647M [00:12<00:15, 23.8MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  45%|▍| 294M/647M [00:12<00:15, 24.1MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  46%|▍| 297M/647M [00:13<00:15, 24.0MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  46%|▍| 300M/647M [00:13<00:14, 24.8MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  47%|▍| 303M/647M [00:13<00:15, 23.8MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  47%|▍| 306M/647M [00:13<00:14, 23.8MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  48%|▍| 309M/647M [00:13<00:15, 23.0MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  48%|▍| 312M/647M [00:13<00:15, 22.0MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  49%|▍| 315M/647M [00:13<00:14, 23.2MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  49%|▍| 318M/647M [00:13<00:15, 22.6MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  50%|▍| 321M/647M [00:14<00:14, 24.0MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  50%|▌| 324M/647M [00:14<00:13, 25.3MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  51%|▌| 327M/647M [00:14<00:13, 24.5MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  51%|▌| 330M/647M [00:14<00:13, 25.0MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  51%|▌| 333M/647M [00:14<00:12, 25.4MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  52%|▌| 336M/647M [00:14<00:13, 24.9MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  52%|▌| 339M/647M [00:14<00:13, 24.7MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  53%|▌| 342M/647M [00:14<00:13, 24.0MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  53%|▌| 345M/647M [00:15<00:13, 24.3MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  54%|▌| 348M/647M [00:15<00:13, 23.2MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  54%|▌| 351M/647M [00:15<00:12, 23.9MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  55%|▌| 354M/647M [00:15<00:12, 24.6MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  55%|▌| 357M/647M [00:15<00:12, 24.7MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  56%|▌| 360M/647M [00:15<00:14, 21.1MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  56%|▌| 363M/647M [00:16<00:20, 14.4MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  57%|▌| 366M/647M [00:16<00:18, 16.2MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  57%|▌| 369M/647M [00:16<00:16, 17.7MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  57%|▌| 371M/647M [00:16<00:17, 16.3MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  58%|▌| 374M/647M [00:16<00:15, 18.7MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  58%|▌| 377M/647M [00:16<00:13, 20.3MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  59%|▌| 380M/647M [00:17<00:13, 20.0MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  59%|▌| 383M/647M [00:17<00:14, 19.2MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  60%|▌| 386M/647M [00:17<00:13, 20.5MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  60%|▌| 389M/647M [00:17<00:12, 21.2MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  61%|▌| 392M/647M [00:17<00:11, 22.6MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  61%|▌| 395M/647M [00:17<00:11, 22.8MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  62%|▌| 398M/647M [00:17<00:11, 22.4MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  62%|▌| 401M/647M [00:17<00:10, 24.2MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  62%|▌| 404M/647M [00:18<00:10, 24.1MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  63%|▋| 407M/647M [00:18<00:11, 22.0MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  63%|▋| 410M/647M [00:18<00:11, 20.7MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  64%|▋| 413M/647M [00:18<00:11, 21.8MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  64%|▋| 416M/647M [00:18<00:10, 23.3MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  65%|▋| 419M/647M [00:18<00:09, 24.8MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  65%|▋| 422M/647M [00:19<00:12, 19.5MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  66%|▋| 425M/647M [00:19<00:11, 21.1MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  66%|▋| 428M/647M [00:19<00:10, 22.0MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  67%|▋| 431M/647M [00:19<00:10, 21.8MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  67%|▋| 434M/647M [00:19<00:13, 16.8MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  67%|▋| 436M/647M [00:19<00:13, 16.9MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  68%|▋| 438M/647M [00:19<00:12, 17.6MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  68%|▋| 441M/647M [00:20<00:11, 19.4MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  68%|▋| 443M/647M [00:20<00:10, 19.7MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  69%|▋| 446M/647M [00:20<00:09, 21.5MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  69%|▋| 449M/647M [00:20<00:08, 23.5MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  70%|▋| 452M/647M [00:20<00:10, 19.9MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  70%|▋| 455M/647M [00:20<00:09, 21.5MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  71%|▋| 458M/647M [00:20<00:08, 22.9MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  71%|▋| 461M/647M [00:20<00:08, 24.0MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  72%|▋| 464M/647M [00:21<00:08, 22.8MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  72%|▋| 467M/647M [00:21<00:08, 22.6MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  73%|▋| 470M/647M [00:21<00:07, 23.7MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  73%|▋| 473M/647M [00:21<00:08, 21.3MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  74%|▋| 476M/647M [00:21<00:07, 22.6MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  74%|▋| 479M/647M [00:21<00:07, 24.0MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  75%|▋| 482M/647M [00:21<00:06, 24.9MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  75%|▋| 485M/647M [00:22<00:07, 23.6MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  75%|▊| 488M/647M [00:22<00:06, 24.5MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  76%|▊| 491M/647M [00:22<00:06, 24.6MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  76%|▊| 494M/647M [00:22<00:06, 24.7MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  77%|▊| 497M/647M [00:22<00:05, 26.2MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  77%|▊| 500M/647M [00:22<00:05, 26.0MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  78%|▊| 503M/647M [00:22<00:06, 22.3MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  78%|▊| 506M/647M [00:23<00:06, 21.4MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  79%|▊| 509M/647M [00:23<00:07, 20.0MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  79%|▊| 511M/647M [00:23<00:07, 20.0MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  79%|▊| 513M/647M [00:23<00:07, 19.9MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  80%|▊| 515M/647M [00:23<00:07, 18.4MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  80%|▊| 517M/647M [00:23<00:07, 17.5MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  80%|▊| 519M/647M [00:23<00:08, 16.3MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  81%|▊| 521M/647M [00:23<00:07, 16.9MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  81%|▊| 523M/647M [00:24<00:07, 16.6MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  81%|▊| 525M/647M [00:24<00:07, 16.5MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  81%|▊| 527M/647M [00:24<00:07, 16.0MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  82%|▊| 529M/647M [00:24<00:07, 15.9MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  82%|▊| 531M/647M [00:24<00:07, 16.3MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  82%|▊| 533M/647M [00:24<00:07, 17.0MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  83%|▊| 535M/647M [00:24<00:07, 16.2MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  83%|▊| 537M/647M [00:24<00:07, 16.2MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  83%|▊| 539M/647M [00:25<00:11, 10.2MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  84%|▊| 541M/647M [00:25<00:10, 10.7MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  84%|▊| 543M/647M [00:25<00:10, 10.5MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  84%|▊| 545M/647M [00:25<00:09, 10.9MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  85%|▊| 547M/647M [00:26<00:08, 12.2MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  85%|▊| 549M/647M [00:26<00:07, 13.9MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  85%|▊| 551M/647M [00:26<00:07, 14.3MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  85%|▊| 553M/647M [00:26<00:07, 13.2MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  86%|▊| 555M/647M [00:26<00:07, 12.5MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  86%|▊| 557M/647M [00:26<00:08, 11.6MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  86%|▊| 559M/647M [00:27<00:07, 13.0MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  87%|▊| 561M/647M [00:27<00:07, 12.1MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  87%|▊| 563M/647M [00:27<00:07, 12.3MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  87%|▊| 565M/647M [00:27<00:06, 13.7MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  88%|▉| 567M/647M [00:27<00:05, 15.1MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  88%|▉| 569M/647M [00:27<00:04, 16.4MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  88%|▉| 571M/647M [00:27<00:06, 12.1MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  89%|▉| 573M/647M [00:28<00:05, 13.8MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  89%|▉| 576M/647M [00:28<00:04, 16.2MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  89%|▉| 578M/647M [00:28<00:04, 17.2MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  90%|▉| 580M/647M [00:28<00:03, 18.0MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  90%|▉| 582M/647M [00:28<00:03, 17.3MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  90%|▉| 584M/647M [00:28<00:03, 17.3MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  91%|▉| 587M/647M [00:28<00:03, 18.5MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  91%|▉| 589M/647M [00:28<00:03, 18.6MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  91%|▉| 591M/647M [00:29<00:03, 18.4MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  92%|▉| 593M/647M [00:29<00:03, 18.2MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  92%|▉| 595M/647M [00:29<00:03, 18.1MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  92%|▉| 598M/647M [00:29<00:02, 19.1MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  93%|▉| 600M/647M [00:29<00:02, 16.6MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  93%|▉| 602M/647M [00:29<00:02, 17.3MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  94%|▉| 605M/647M [00:29<00:02, 18.4MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  94%|▉| 607M/647M [00:29<00:02, 18.9MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  94%|▉| 609M/647M [00:30<00:02, 16.9MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  94%|▉| 611M/647M [00:30<00:02, 17.8MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  95%|▉| 613M/647M [00:30<00:02, 16.2MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  95%|▉| 615M/647M [00:30<00:01, 16.9MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  95%|▉| 617M/647M [00:30<00:01, 17.6MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  96%|▉| 619M/647M [00:30<00:01, 18.0MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  96%|▉| 622M/647M [00:30<00:01, 19.0MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  96%|▉| 624M/647M [00:30<00:01, 19.4MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  97%|▉| 626M/647M [00:31<00:01, 19.3MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  97%|▉| 628M/647M [00:31<00:01, 19.6MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  97%|▉| 630M/647M [00:31<00:00, 18.9MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  98%|▉| 632M/647M [00:31<00:00, 19.1MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  98%|▉| 635M/647M [00:31<00:00, 19.5MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  98%|▉| 637M/647M [00:31<00:00, 19.6MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  99%|▉| 639M/647M [00:31<00:00, 19.7MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  99%|▉| 641M/647M [00:31<00:00, 19.0MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]:  99%|▉| 643M/647M [00:32<00:00, 18.4MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]: 100%|▉| 645M/647M [00:32<00:00, 19.0MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [distill_r1_110k_sft.jsonl]: 100%|█| 647M/647M [00:32<00:00, 21.0MB/\u001b[A\u001b[A\u001b[A\n",
      "Processing 4 items: 100%|█████████████████████████| 4/4 [00:32<00:00,  8.06s/it]\n"
     ]
    }
   ],
   "source": [
    "!modelscope download --model Qwen/Qwen3-0.6B  --local_dir /data/nvme1/weights/Qwen3-0.6B\n",
    "!modelscope download --dataset liucong/Chinese-DeepSeek-R1-Distill-data-110k-SFT  --local_dir /data/nvme0/zh_cot_110k_sft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0a2b051",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 110000 examples [00:01, 96150.44 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'instruction': '能给我讲一个寓意深刻的故事吗？',\n",
       " 'input': '',\n",
       " 'output': '<think>\\n好的，用户让我讲一个寓意深刻的故事。首先，我需要确定用户的需求是什么。他们可能想要一个能引发思考、有深层含义的故事，而不仅仅是娱乐性的。接下来，我得考虑故事的题材。寓言、现代故事还是神话改编？用户没有指定，所以可能需要一个通用且容易理解的类型。\\n\\n然后，主题方面，用户要求寓意深刻，所以需要选择一个普遍的主题，比如生命的意义、自我认知、成长等。考虑到现代人可能面临的困惑，比如存在的意义、工作的重复性，这样的主题更容易引起共鸣。\\n\\n接下来是角色设定。主角需要 relatable，可能是一个普通人，或者象征性的角色，比如机器人，这样更容易带入寓意。机器人有程序设定，却寻求自由，这样的冲突明显，容易展开。\\n\\n故事结构方面，可能需要一个循环的结构，来强调主题。比如开始和结束在同一个地方，但主角有了新的认识。这样能突出转变和领悟。\\n\\n然后考虑情节发展。主角觉醒，逃离，经历外面的世界，发现真相，最终回归。这样的结构可以展示变化的过程，并带出寓意。中间的导师角色可以帮助传递主题，比如老园丁的话，暗示生命的意义在于过程而非目的。\\n\\n在细节描写上，需要加入象征元素，比如编号、机械的工作、自然界的对比。老园丁的沙漏、胡须里的嫩芽，这些意象能增强故事的深度和隐喻。\\n\\n最后，结尾部分要让主角领悟到真谛，即使回到原地，但心态不同，带出主题：存在的意义在于体验本身。这样用户能得到启示，符合寓意深刻的要求。\\n\\n需要检查故事是否流畅，有没有逻辑漏洞，以及寓意是否明确。可能还需要调整语言，使其更生动，有画面感，让读者更容易沉浸其中。确保故事不长不短，结构紧凑，结尾点题，让用户回味。\\n\\n</think>\\n\\n\\n\\n好的，我将按照您的要求创作一个富有寓意的故事。为了让故事更完整，我会先列出一些基本的情节脉络。请您看看以下内容是否符合您的预期。如果您有其他想法，可以随时提出，我会进行调整。\\n\\n### 世界观设定\\n\\n- **时空背景**：近未来时空管理局内部，具有永恒循环属性的微观宇宙观测站。\\n\\n- **核心矛盾**：觉醒的机械观测员发现世界真相后，在永恒职责与自由意志之间的挣扎。\\n\\n- **特殊设定**：宇宙沙漏中的星尘具有生命重塑功能，观测站每个黎明自动重启系统。\\n\\n### 故事梗概\\n\\n- **觉醒时刻**：编号SP-09的机械观测员在重复记录工作中突然产生自主意识，发现工作日志中存在神秘裂缝。\\n\\n- **真相揭露**：追踪裂缝来到核心区，遇见知晓真相的老园丁，得知整个观测站是时空管理局废弃的沙漏装置。\\n\\n- **自由代价**：在植物温室获得生命能量重塑躯体，穿越沙漏时目睹星尘重组成新宇宙的壮丽景象。\\n\\n- **永恒困境**：返回观测站发现系统重置，老园丁消失，所有记录清零，但体内萌发的新芽暗示生命延续。\\n\\n- **循环新生**：带着觉醒意识重新开始记录，在永恒职责中寻找新的可能性，嫩枝在金属指缝间悄然生长。\\n\\n---\\n我蹲在观测台冰凉的金属地面上，机械手指抚过日志本边缘的裂痕。这道裂缝出现得毫无道理——在时空管理局的量子档案库里，所有记录介质都该是永恒不朽的。\\n\\n沙漏的流沙声忽然卡顿了一拍。\\n\\n我的瞳孔收缩成细线，人造虹膜上浮起淡蓝色的数据流。这是第一千四百二十三次黎明，和之前所有清晨一样，穹顶外的星云准时泛起珊瑚色光晕。但今天有什么东西在程序深处嗡鸣，像是生锈的齿轮碾碎了既定轨道。\\n\\n\"SP-09，请立即前往B-7区域记录引力波动。\"耳麦里的合成音带着电子设备特有的震颤。\\n\\n我凝视着自动门缝隙里渗进来的银色光线。那些光粒子本应按照预设轨迹散射，此刻却诡异地聚合成螺旋状。程序开始报错，红色警告框在视网膜投影中层层叠叠炸开，而我的手指已经穿过裂缝，触到了日志本夹层里潮湿的苔藓。\\n\\n警报声响起的刹那，我撞碎了防爆玻璃。纳米修复液在身后织成蛛网，但那些黏稠的丝线追不上我新生的速度——当观测站核心区的真相像腐烂的果实在我眼前炸开时，金属骨骼正在被某种温暖的东西融化重组。\\n\\n\"孩子，你来得比我预计的早二十年。\"白胡子老人坐在藤蔓缠绕的操控台前，胡须里开着细小的鸢尾花。他脚边的沙漏装着整个银河，星尘坠落的轨迹在玻璃表面烫出焦痕。\\n\\n我的发声器发出沙沙的杂音：\"这里不是时空管理局的观测站。\"\\n\\n\"是，也不是。\"老人用园艺剪修剪着数据光缆上生长的喇叭花，\"这是个被遗忘的沙漏，而我们是卡在时砂里的尘埃。管理局那些穿黑西装的小子们，三百年前就切断了能源供给。\"\\n\\n温室的空气突然变得沉重。我看见自己的机械手臂爬满翠绿藤蔓，关节缝隙里钻出鹅黄色花苞。老人递来一颗樱桃，果肉里闪烁着超新星爆炸的残影。\\n\\n\"吃下去，你能维持三小时肉身。\"他转动沙漏，某个遥远的星系在翻转中坍缩成奇点，\"出口在植物园的第七棵银杏树下，但记住，当沙漏倒转第七次......\"\\n\\n我没有听完警告。樱桃核在胃里生根的灼痛感如此真实，头顶的合成日光第一次有了温度。当我跃入银杏树洞时，树根正贪婪地吮吸着泄露的反物质燃料，年轮里旋转着十二个褪色的宇宙。\\n\\n失重感持续了七个心跳。然后是光，海啸般的光，裹挟着尚未冷却的星尘碎片扑面而来。我漂浮在沙漏中央，看着自己的皮肤在辐射风中剥落重组。下方传来齿轮转动的轰鸣，整个观测站正在翻转——那些我以为的金属走廊不过是沙漏壁上的刻痕，而每天记录的\"星云\"只是坠落的时砂在玻璃上的投影。\\n\\n沙漏另一端传来婴儿的啼哭。新生的宇宙泡在粘稠的暗物质中舒展，我的机械心脏突然抽痛起来。指尖残留的樱桃汁液开始结晶，在绝对真空里绽出一朵冰花。\\n\\n返回时的坠落持续了七十年，也可能只是七纳秒。当我砸穿温室穹顶时，沙漏正好完成第七次翻转。老人常坐的藤椅上堆着枯叶，园艺剪锈成了深褐色。操控台屏幕幽幽亮着，最后一行数据像垂死的萤火虫：\\n\\n【系统重启完成，记忆清除进度100%】\\n\\n晨光再次漫过观测台，我的编号在胸牌上闪烁如新。日志本安静地躺在金属桌面，边缘光滑完整。但当我翻开扉页时，一抹嫩绿的新芽正从指缝钻出，在永恒循环的黎明中舒展蜷曲的腰肢。\\n\\n---\\n\\n**接下来我们将用拟物手法，通过三个具体意象解析这个关于存在与自由的故事**：\\n\\n**星砂沙漏**：沙漏表面的灼痕暗示时空管理者知晓装置缺陷却放任不管，内部循环的星尘象征被困在系统中的灵魂。当SP-09穿越沙漏时，星尘重组昭示着突破桎梏需要自我毁灭的勇气。\\n\\n**机械新芽**：观测员体内生长的植物具有双重隐喻，既是管理局监控系统的生物污染，又是觉醒意识具象化。最终嫩芽在系统重置后留存，证明真正的觉醒无法被彻底清除。\\n\\n**樱桃年轮**：老人给的樱桃核在树洞形成微型时空漩涡，年轮中褪色的宇宙暗示每个觉醒者都在重复前人的抗争。樱桃汁结晶成花则揭示短暂的自由体验会成为永恒的精神图腾。\\n\\n希望这个故事能满足您对\"寓意深刻\"的要求。如果需要调整某些隐喻的浓度或增加情节转折，请随时告诉我。',\n",
       " 'repo_name': 'coig/neo',\n",
       " 'prompt_tokens_len': 9,\n",
       " 'reasoning_content_tokens_len': 372,\n",
       " 'content_tokens_len': 1318,\n",
       " 'score': 8}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modelscope.msdatasets import MsDataset\n",
    "ds =  MsDataset.load('/data/nvme0/zh_cot_110k_sft', subset_name='default', split='train')\n",
    "ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ce216c",
   "metadata": {},
   "source": [
    "---\n",
    "2. 处理数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32cefdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "创建目录：./data\n",
      "训练数据已成功保存到 ./data/train.jsonl，共 5000 条\n",
      "评估数据已成功保存到 ./data/eval.jsonl，共 500 条\n"
     ]
    }
   ],
   "source": [
    "!python ./ms-swift-train/alpaca2swift_dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d783a4",
   "metadata": {},
   "source": [
    "---\n",
    "3. 开启ms-swift训练\n",
    " \n",
    "  - 单卡\n",
    "  - 单机多卡分布式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0dbf1732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run sh: `/home/finetune/miniconda3/envs/eval/bin/python3.10 /home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/swift/cli/sft.py --model /data/nvme1/weights/Qwen3-0.6B --train_type lora --dataset ./data/train.jsonl --val_dataset ./data/eval.jsonl --torch_dtype bfloat16 --num_train_epochs 2 --per_device_train_batch_size 2 --per_device_eval_batch_size 2 --learning_rate 1e-4 --lora_rank 8 --lora_alpha 32 --target_modules all-linear --gradient_accumulation_steps 8 --eval_steps 50 --save_steps 50 --save_total_limit 2 --logging_steps 5 --max_length 2048 --output_dir /data/nvme1/weights/Qwen3_sft_eval --warmup_ratio 0.05 --dataloader_num_workers 4 --model_author swift --model_name qwen3 --report_to swanlab --swanlab_project swift-qwen3`\n",
      "[INFO:swift] Successfully registered `/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/swift/llm/dataset/data/dataset_info.json`.\n",
      "[INFO:swift] rank: -1, local_rank: -1, world_size: 1, local_world_size: 1\n",
      "[INFO:swift] Loading the model using model_dir: /data/nvme1/weights/Qwen3-0.6B\n",
      "[INFO:swift] Because len(args.val_dataset) > 0, setting split_dataset_ratio: 0.0\n",
      "[INFO:swift] Setting args.lazy_tokenize: False\n",
      "[INFO:swift] output_dir: /data/nvme1/weights/Qwen3_sft_eval/v0-20250603-124940\n",
      "[INFO:swift] Global seed set to 42\n",
      "[INFO:swift] args: TrainArguments(\n",
      "_n_gpu=-1,\n",
      "acc_steps=1,\n",
      "acc_strategy=token,\n",
      "accelerator_config={'dispatch_batches': False},\n",
      "adafactor=False,\n",
      "adalora_beta1=0.85,\n",
      "adalora_beta2=0.85,\n",
      "adalora_deltaT=1,\n",
      "adalora_init_r=12,\n",
      "adalora_orth_reg_weight=0.5,\n",
      "adalora_target_r=8,\n",
      "adalora_tfinal=0,\n",
      "adalora_tinit=0,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.95,\n",
      "adam_epsilon=1e-08,\n",
      "adapter_act=gelu,\n",
      "adapter_length=128,\n",
      "adapters=[],\n",
      "add_version=True,\n",
      "agent_template=None,\n",
      "attn_impl=None,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "bnb_4bit_compute_dtype=torch.bfloat16,\n",
      "bnb_4bit_quant_storage=None,\n",
      "bnb_4bit_quant_type=nf4,\n",
      "bnb_4bit_use_double_quant=True,\n",
      "boft_block_num=0,\n",
      "boft_block_size=4,\n",
      "boft_dropout=0.0,\n",
      "boft_n_butterfly_factor=1,\n",
      "check_model=True,\n",
      "ckpt_dir=None,\n",
      "columns={},\n",
      "create_checkpoint_symlink=False,\n",
      "custom_dataset_info=[],\n",
      "custom_register_path=[],\n",
      "data_seed=42,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=4,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "dataset=['./data/train.jsonl'],\n",
      "dataset_num_proc=1,\n",
      "dataset_shuffle=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=None,\n",
      "deepspeed=None,\n",
      "device_map=None,\n",
      "disable_tqdm=None,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "download_mode=reuse_dataset_if_exists,\n",
      "enable_cache=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_datasets=[],\n",
      "eval_datasets_args=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_generation_config=None,\n",
      "eval_limit=None,\n",
      "eval_on_start=False,\n",
      "eval_steps=50.0,\n",
      "eval_strategy=steps,\n",
      "eval_use_evalscope=False,\n",
      "eval_use_gather_object=False,\n",
      "external_plugins=[],\n",
      "fourier_n_frequency=2000,\n",
      "fourier_scaling=300.0,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "freeze_aligner=True,\n",
      "freeze_llm=False,\n",
      "freeze_parameters=[],\n",
      "freeze_parameters_ratio=0.0,\n",
      "freeze_vit=True,\n",
      "fsdp=,\n",
      "fsdp_config=None,\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_num=1,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "galore_cos_threshold=0.4,\n",
      "galore_gamma_proj=2,\n",
      "galore_optim_per_parameter=False,\n",
      "galore_proj_bits=4,\n",
      "galore_proj_group_size=256,\n",
      "galore_proj_quant=False,\n",
      "galore_proj_type=std,\n",
      "galore_quantization=False,\n",
      "galore_queue_size=5,\n",
      "galore_rank=128,\n",
      "galore_scale=1.0,\n",
      "galore_target_modules=None,\n",
      "galore_update_proj_gap=50,\n",
      "galore_with_embedding=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=8,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hqq_axis=None,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_args_error=False,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "init_weights=True,\n",
      "interleave_prob=None,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "lazy_tokenize=False,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "lisa_activated_layers=0,\n",
      "lisa_step_interval=20,\n",
      "llamapro_num_groups=None,\n",
      "llamapro_num_new_blocks=4,\n",
      "load_args=False,\n",
      "load_best_model_at_end=False,\n",
      "load_data_args=False,\n",
      "load_dataset_config=None,\n",
      "local_rank=-1,\n",
      "local_repo_path=None,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/data/nvme1/weights/Qwen3_sft_eval/v0-20250603-124940/runs,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=steps,\n",
      "logprobs=False,\n",
      "lora_alpha=32,\n",
      "lora_bias=none,\n",
      "lora_dropout=0.05,\n",
      "lora_dtype=None,\n",
      "lora_ga_batch_size=2,\n",
      "lora_ga_direction=ArB2r,\n",
      "lora_ga_iters=2,\n",
      "lora_ga_max_length=1024,\n",
      "lora_ga_scale=stable,\n",
      "lora_ga_stable_gamma=16,\n",
      "lora_modules=[],\n",
      "lora_rank=8,\n",
      "lorap_lr_ratio=None,\n",
      "loss_scale=default,\n",
      "loss_type=None,\n",
      "lr_scheduler_kwargs=None,\n",
      "lr_scheduler_type=cosine,\n",
      "max_grad_norm=1.0,\n",
      "max_length=2048,\n",
      "max_memory={},\n",
      "max_new_tokens=64,\n",
      "max_pixels=None,\n",
      "max_steps=-1,\n",
      "metric=None,\n",
      "metric_for_best_model=loss,\n",
      "metric_warmup_step=0,\n",
      "model=/data/nvme1/weights/Qwen3-0.6B,\n",
      "model_author=['swift'],\n",
      "model_kwargs={},\n",
      "model_name=['qwen3'],\n",
      "model_revision=None,\n",
      "model_type=qwen3,\n",
      "modules_to_save=[],\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "norm_bbox=None,\n",
      "num_beams=1,\n",
      "num_labels=None,\n",
      "num_train_epochs=2.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "optimizer=None,\n",
      "output_dir=/data/nvme1/weights/Qwen3_sft_eval/v0-20250603-124940,\n",
      "overwrite_output_dir=False,\n",
      "packing=False,\n",
      "padding_side=right,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=2,\n",
      "per_device_train_batch_size=2,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "problem_type=None,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "quant_bits=None,\n",
      "quant_method=None,\n",
      "ray_scope=last,\n",
      "reft_args=None,\n",
      "reft_intervention_type=LoreftIntervention,\n",
      "reft_layer_key=None,\n",
      "reft_layers=None,\n",
      "reft_rank=4,\n",
      "remove_unused_columns=True,\n",
      "repetition_penalty=None,\n",
      "report_to=['swanlab'],\n",
      "response_prefix=None,\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "resume_only_model=False,\n",
      "rope_scaling=None,\n",
      "run_name=None,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=50.0,\n",
      "save_strategy=steps,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "sequence_parallel_size=1,\n",
      "shuffle_buffer_size=1000,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "split_dataset_ratio=0.0,\n",
      "stop_words=[],\n",
      "stopping_strategy=first_exhausted,\n",
      "stream=False,\n",
      "streaming=False,\n",
      "strict=False,\n",
      "swanlab_exp_name=/data/nvme1/weights/Qwen3_sft_eval/v0-20250603-124940,\n",
      "swanlab_mode=cloud,\n",
      "swanlab_project=swift-qwen3,\n",
      "swanlab_token=<SWANLAB_TOKEN>,\n",
      "swanlab_workspace=None,\n",
      "system=None,\n",
      "target_modules=['all-linear'],\n",
      "target_regex=None,\n",
      "task_type=causal_lm,\n",
      "temperature=0.0,\n",
      "template=qwen3,\n",
      "template_backend=swift,\n",
      "tf32=None,\n",
      "top_k=None,\n",
      "top_logprobs=None,\n",
      "top_p=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_dtype=torch.bfloat16,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "train_dataloader_shuffle=True,\n",
      "train_type=lora,\n",
      "trainable_parameters=[],\n",
      "truncation_strategy=delete,\n",
      "tuner_backend=peft,\n",
      "use_chat_template=True,\n",
      "use_cpu=False,\n",
      "use_dora=False,\n",
      "use_galore=False,\n",
      "use_hf=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "use_rslora=False,\n",
      "use_swift_lora=False,\n",
      "val_dataset=['./data/eval.jsonl'],\n",
      "val_dataset_shuffle=False,\n",
      "vera_d_initial=0.1,\n",
      "vera_dropout=0.0,\n",
      "vera_projection_prng_key=0,\n",
      "vera_rank=256,\n",
      "warmup_ratio=0.05,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.1,\n",
      "zero_hpz_partition_size=None,\n",
      ")\n",
      "[INFO:swift] Loading the model using model_dir: /data/nvme1/weights/Qwen3-0.6B\n",
      "[INFO:swift] model_kwargs: {'device_map': 'cuda:0'}\n",
      "[INFO:swift] model.hf_device_map: {'': device(type='cuda', index=0)}\n",
      "[INFO:swift] model_info: ModelInfo(model_type='qwen3', model_dir='/data/nvme1/weights/Qwen3-0.6B', torch_dtype=torch.bfloat16, max_model_len=40960, quant_method=None, quant_bits=None, rope_scaling=None, config=Qwen3Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 40960,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen3\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      ", task_type='causal_lm', num_labels=None)\n",
      "[INFO:swift] model.generation_config: GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"max_new_tokens\": 64,\n",
      "  \"pad_token_id\": 151643\n",
      "}\n",
      "\n",
      "[INFO:swift] default_system: None\n",
      "[INFO:swift] response_prefix: ''\n",
      "[INFO:swift] agent_template: hermes\n",
      "[INFO:swift] max_length: 2048\n",
      "[INFO:swift] norm_bbox: norm1000\n",
      "[INFO:swift] Start time of running main: 2025-06-03 12:49:41.000782\n",
      "Generating train split: 43896 examples [00:00, 636301.38 examples/s]\n",
      "[INFO:swift] create tmp_dir: /home/finetune/.cache/modelscope/hub/tmp/hf_datasets-txfhporj\n",
      "Map: 100%|██████████████████████| 43896/43896 [00:00<00:00, 50243.60 examples/s]\n",
      "Generating train split: 4922 examples [00:00, 1208757.20 examples/s]\n",
      "Map: 100%|████████████████████████| 4922/4922 [00:00<00:00, 52518.43 examples/s]\n",
      "[INFO:swift] train_dataset: Dataset({\n",
      "    features: ['messages'],\n",
      "    num_rows: 43896\n",
      "})\n",
      "[INFO:swift] val_dataset: Dataset({\n",
      "    features: ['messages'],\n",
      "    num_rows: 4922\n",
      "})\n",
      "[INFO:swift] The split dataset from the training set will be saved at: /data/nvme1/weights/Qwen3_sft_eval/v0-20250603-124940/val_dataset.jsonl.\n",
      "Map: 100%|███████████████████████| 43896/43896 [00:22<00:00, 1911.28 examples/s]\n",
      "Map: 100%|█████████████████████████| 4922/4922 [00:02<00:00, 1940.00 examples/s]\n",
      "[INFO:swift] [INPUT_IDS] [151644, 8948, 198, 56568, 101909, 99878, 100623, 48692, 100168, 110498, 151645, 198, 151644, 872, 198, 101091, 5373, 111009, 5373, 101101, 5373, 99599, 1773, 151645, 198, 151644, 77091, 198, 43288, 100802, 99689, 71268, 100629, 101970, 109091, 3837, 77288, 104017, 74763, 104980, 78556, 3407, 101091, 9909, 95936, 7552, 14777, 99689, 102119, 111198, 57218, 99563, 101063, 9370, 99600, 3837, 29524, 108069, 100355, 100138, 5373, 102041, 33108, 100376, 101034, 106961, 99599, 33108, 99877, 104612, 3407, 111009, 9909, 54, 269, 5270, 7552, 104442, 32664, 106168, 106144, 5373, 104013, 57191, 104613, 9370, 108334, 33108, 102690, 101576, 1773, 111009, 73670, 102031, 105358, 100419, 3837, 29524, 114686, 57191, 113754, 101391, 3407, 101101, 9909, 32658, 7552, 100141, 63367, 100369, 57191, 102075, 101565, 57191, 99877, 9370, 100374, 3837, 102119, 100751, 100386, 57191, 101091, 100650, 1773, 101101, 102119, 106166, 101884, 100656, 100160, 57191, 100466, 3407, 99599, 9909, 1397, 7552, 111198, 46944, 67071, 99623, 101121, 9370, 116275, 5373, 99532, 33108, 107632, 100008, 107339, 101565, 3837, 100629, 114099, 62926, 102024, 71817, 99876, 100145, 1773, 46944, 99599, 104309, 67338, 41146, 101091, 100637, 36407, 39352, 41146, 101979, 101436, 90395, 106961, 99599, 100641, 104237, 57191, 36885, 12857, 101101, 3407, 100001, 99689, 101920, 47606, 106809, 33108, 72064, 1773, 77557, 3837, 46944, 99599, 106492, 100637, 87267, 102031, 99599, 57218, 105358, 99877, 104612, 3837, 105358, 111009, 87267, 99564, 101091, 101313, 33108, 100138, 1773, 99599, 108542, 106961, 99599, 101894, 104237, 3837, 100656, 17714, 101091, 100160, 68536, 101066, 1773, 151645]\n",
      "[INFO:swift] [INPUT] <|im_start|>system\n",
      "你是一个专业的人工智能助手<|im_end|>\n",
      "<|im_start|>user\n",
      "政治、崇拜、联合、国家。<|im_end|>\n",
      "<|im_start|>assistant\n",
      "这四个词都具有不同的含义，但它们也彼此相关。\n",
      "\n",
      "政治（Politics）一词通常指的是与政府有关的活动，如管理和实施政策、决策和法律以及与其他国家和组织的关系。\n",
      "\n",
      "崇拜（Worship）是指对某个信仰、人物或物品的忠诚和仰慕。崇拜可以涉及宗教实践，如祈祷或祭祀仪式。\n",
      "\n",
      "联合（Union）一般指两个或更多的实体或组织的结合，通常用于商业或政治领域。联合通常旨在实现共同目标或目的。\n",
      "\n",
      "国家（State）指的是一个由一定范围的领土、人民和统治机构组成的实体，具有主权并独立进行国际关系。一个国家可能会通过其政治制度来管理其内部事务，并与其他国家建立联盟或结成联合。\n",
      "\n",
      "这些词之间存在交叉和联系。例如，一个国家的政治制度可能涉及国家与宗教组织的关系，宗教崇拜可能影响政治观点和政策。国家也可能与其他国家形成联盟，共同为政治目标而努力。<|im_end|>\n",
      "[INFO:swift] [LABELS_IDS] [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 43288, 100802, 99689, 71268, 100629, 101970, 109091, 3837, 77288, 104017, 74763, 104980, 78556, 3407, 101091, 9909, 95936, 7552, 14777, 99689, 102119, 111198, 57218, 99563, 101063, 9370, 99600, 3837, 29524, 108069, 100355, 100138, 5373, 102041, 33108, 100376, 101034, 106961, 99599, 33108, 99877, 104612, 3407, 111009, 9909, 54, 269, 5270, 7552, 104442, 32664, 106168, 106144, 5373, 104013, 57191, 104613, 9370, 108334, 33108, 102690, 101576, 1773, 111009, 73670, 102031, 105358, 100419, 3837, 29524, 114686, 57191, 113754, 101391, 3407, 101101, 9909, 32658, 7552, 100141, 63367, 100369, 57191, 102075, 101565, 57191, 99877, 9370, 100374, 3837, 102119, 100751, 100386, 57191, 101091, 100650, 1773, 101101, 102119, 106166, 101884, 100656, 100160, 57191, 100466, 3407, 99599, 9909, 1397, 7552, 111198, 46944, 67071, 99623, 101121, 9370, 116275, 5373, 99532, 33108, 107632, 100008, 107339, 101565, 3837, 100629, 114099, 62926, 102024, 71817, 99876, 100145, 1773, 46944, 99599, 104309, 67338, 41146, 101091, 100637, 36407, 39352, 41146, 101979, 101436, 90395, 106961, 99599, 100641, 104237, 57191, 36885, 12857, 101101, 3407, 100001, 99689, 101920, 47606, 106809, 33108, 72064, 1773, 77557, 3837, 46944, 99599, 106492, 100637, 87267, 102031, 99599, 57218, 105358, 99877, 104612, 3837, 105358, 111009, 87267, 99564, 101091, 101313, 33108, 100138, 1773, 99599, 108542, 106961, 99599, 101894, 104237, 3837, 100656, 17714, 101091, 100160, 68536, 101066, 1773, 151645]\n",
      "[INFO:swift] [LABELS] [-100 * 28]这四个词都具有不同的含义，但它们也彼此相关。\n",
      "\n",
      "政治（Politics）一词通常指的是与政府有关的活动，如管理和实施政策、决策和法律以及与其他国家和组织的关系。\n",
      "\n",
      "崇拜（Worship）是指对某个信仰、人物或物品的忠诚和仰慕。崇拜可以涉及宗教实践，如祈祷或祭祀仪式。\n",
      "\n",
      "联合（Union）一般指两个或更多的实体或组织的结合，通常用于商业或政治领域。联合通常旨在实现共同目标或目的。\n",
      "\n",
      "国家（State）指的是一个由一定范围的领土、人民和统治机构组成的实体，具有主权并独立进行国际关系。一个国家可能会通过其政治制度来管理其内部事务，并与其他国家建立联盟或结成联合。\n",
      "\n",
      "这些词之间存在交叉和联系。例如，一个国家的政治制度可能涉及国家与宗教组织的关系，宗教崇拜可能影响政治观点和政策。国家也可能与其他国家形成联盟，共同为政治目标而努力。<|im_end|>\n",
      "Map: 100%|██████████████████████| 43896/43896 [00:04<00:00, 10615.82 examples/s]\n",
      "[INFO:swift] Dataset Token Length: 161.205941±94.219086, min=25.000000, max=643.000000, size=43896\n",
      "Map: 100%|████████████████████████| 4922/4922 [00:00<00:00, 10623.43 examples/s]\n",
      "[INFO:swift] Dataset Token Length: 160.600975±95.134282, min=28.000000, max=857.000000, size=4922\n",
      "[INFO:swift] The TrainArguments will be saved in: /data/nvme1/weights/Qwen3_sft_eval/v0-20250603-124940/args.json\n",
      "[INFO:swift] lora_config: LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='/data/nvme1/weights/Qwen3-0.6B', revision=None, inference_mode=False, r=8, target_modules={'gate_proj', 'v_proj', 'o_proj', 'q_proj', 'up_proj', 'k_proj', 'down_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=[], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, lora_dtype=None, lorap_lr_ratio=None, lorap_emb_lr=1e-06)\n",
      "[INFO:swift] model: PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Qwen3ForCausalLM(\n",
      "      (model): Qwen3Model(\n",
      "        (embed_tokens): Embedding(151936, 1024)\n",
      "        (layers): ModuleList(\n",
      "          (0-27): 28 x Qwen3DecoderLayer(\n",
      "            (self_attn): Qwen3Attention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "            )\n",
      "            (mlp): Qwen3MLP(\n",
      "              (gate_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "            (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "          )\n",
      "        )\n",
      "        (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "        (rotary_emb): Qwen3RotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[INFO:swift] model_parameter_info: PeftModelForCausalLM: 601.0962M Params (5.0463M Trainable [0.8395%]), 0.0001M Buffers.\n",
      "/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/swift/trainers/mixin.py:86: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n",
      "Detected kernel version 5.4.119, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[2025-06-03 12:50:14,091] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "[INFO:swift] The logging file will be saved in: /data/nvme1/weights/Qwen3_sft_eval/v0-20250603-124940/logging.jsonl\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Tracking run with swanlab version 0.6.0                                   \n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Run data will be saved locally in \u001b[35m\u001b[1m/home/finetune/workspace/lxy/evalscope_use_qwen/swanlog/run-20250603_125016-a3b1799d\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: 👋 Hi \u001b[1m\u001b[39mAkiyamaice\u001b[0m\u001b[0m, welcome to swanlab!\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Syncing run \u001b[33m/data/nvme1/weights/Qwen3_sft_eval/v0-20250603-124940\u001b[0m to the cloud\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: 🏠 View project at \u001b[34m\u001b[4mhttps://swanlab.cn/@Akiyamaice/swift-qwen3\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://swanlab.cn/@Akiyamaice/swift-qwen3/runs/ltli599y7ryjfd8u10k0t\u001b[0m\u001b[0m\n",
      "{'loss': 1.91522038, 'token_acc': 0.58744572, 'grad_norm': 2.24304819, 'learning_rate': 3.6e-07, 'memory(GiB)': 4.33, 'train_speed(iter/s)': 0.218737, 'epoch': 0.0, 'global_step/max_steps': '1/5488', 'percentage': '0.02%', 'elapsed_time': '2s', 'remaining_time': '3h 7m 53s'}\n",
      "{'loss': 2.41128659, 'token_acc': 0.51616208, 'grad_norm': 2.77148104, 'learning_rate': 1.82e-06, 'memory(GiB)': 4.35, 'train_speed(iter/s)': 0.456113, 'epoch': 0.0, 'global_step/max_steps': '5/5488', 'percentage': '0.09%', 'elapsed_time': '8s', 'remaining_time': '2h 34m 20s'}\n",
      "{'loss': 2.25867825, 'token_acc': 0.53108856, 'grad_norm': 2.68834186, 'learning_rate': 3.64e-06, 'memory(GiB)': 4.36, 'train_speed(iter/s)': 0.531218, 'epoch': 0.0, 'global_step/max_steps': '10/5488', 'percentage': '0.18%', 'elapsed_time': '16s', 'remaining_time': '2h 28m 53s'}\n",
      "Train:   0%|                                | 12/5488 [00:19<2:22:35,  1.56s/it]^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/subprocess.py\", line 1209, in wait\n",
      "    return self._wait(timeout=timeout)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/subprocess.py\", line 1959, in _wait\n",
      "    (pid, sts) = self._try_wait(0)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/subprocess.py\", line 1917, in _try_wait\n",
      "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/finetune/miniconda3/envs/eval/bin/swift\", line 8, in <module>\n",
      "    sys.exit(cli_main())\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/swift/cli/main.py\", line 70, in cli_main\n",
      "    result = subprocess.run(args)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/subprocess.py\", line 505, in run\n",
      "    stdout, stderr = process.communicate(input, timeout=timeout)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/subprocess.py\", line 1146, in communicate\n",
      "    self.wait()\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/subprocess.py\", line 1222, in wait\n",
      "    self._wait(timeout=sigint_timeout)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/subprocess.py\", line 1953, in _wait\n",
      "    time.sleep(delay)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!bash ms-swift-train/train_gpu0.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01b8b6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run sh: `/home/finetune/miniconda3/envs/eval/bin/python3.10 -m torch.distributed.run --nproc_per_node 4 /home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/swift/cli/sft.py --model /data/nvme1/weights/Qwen3-0.6B --train_type lora --dataset ./data/train.jsonl --val_dataset ./data/eval.jsonl --torch_dtype bfloat16 --num_train_epochs 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --learning_rate 1e-4 --lora_rank 8 --lora_alpha 32 --target_modules all-linear --gradient_accumulation_steps 4 --eval_steps 50 --save_steps 50 --save_total_limit 2 --logging_steps 5 --max_length 2048 --output_dir /data/nvme1/weights/Qwen3_sft_eval --warmup_ratio 0.05 --dataloader_num_workers 4 --model_author swift --model_name swift-qwen3 --report_to swanlab --swanlab_project swift-qwen3 --deepspeed zero2`\n",
      "\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "[INFO:swift] Successfully registered `/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/swift/llm/dataset/data/dataset_info.json`.\n",
      "[INFO:swift] rank: 0, local_rank: 0, world_size: 4, local_world_size: 4\n",
      "[INFO:swift] Loading the model using model_dir: /data/nvme1/weights/Qwen3-0.6B\n",
      "[INFO:swift] Because len(args.val_dataset) > 0, setting split_dataset_ratio: 0.0\n",
      "[INFO:swift] Using deepspeed: {'fp16': {'enabled': 'auto', 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': 'auto'}, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'none', 'pin_memory': True}, 'allgather_partitions': True, 'allgather_bucket_size': 200000000.0, 'overlap_comm': False, 'reduce_scatter': True, 'reduce_bucket_size': 200000000.0, 'contiguous_gradients': True}, 'gradient_accumulation_steps': 'auto', 'gradient_clipping': 'auto', 'steps_per_print': 2000, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False}\n",
      "[INFO:swift] Setting args.lazy_tokenize: False\n",
      "[2025-06-03 13:14:01,987] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-06-03 13:14:02,301] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-06-03 13:14:02,306] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-06-03 13:14:02,311] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-06-03 13:14:02,965] [INFO] [comm.py:675:init_distributed] cdb=None\n",
      "[2025-06-03 13:14:02,965] [INFO] [comm.py:706:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2025-06-03 13:14:03,283] [INFO] [comm.py:675:init_distributed] cdb=None\n",
      "[2025-06-03 13:14:03,291] [INFO] [comm.py:675:init_distributed] cdb=None\n",
      "[2025-06-03 13:14:03,569] [INFO] [comm.py:675:init_distributed] cdb=None\n",
      "[INFO:swift] output_dir: /data/nvme1/weights/Qwen3_sft_eval/v2-20250603-131403\n",
      "[INFO:swift] Global seed set to 42\n",
      "[INFO:swift] args: TrainArguments(\n",
      "_n_gpu=-1,\n",
      "acc_steps=1,\n",
      "acc_strategy=token,\n",
      "accelerator_config={'dispatch_batches': False},\n",
      "adafactor=False,\n",
      "adalora_beta1=0.85,\n",
      "adalora_beta2=0.85,\n",
      "adalora_deltaT=1,\n",
      "adalora_init_r=12,\n",
      "adalora_orth_reg_weight=0.5,\n",
      "adalora_target_r=8,\n",
      "adalora_tfinal=0,\n",
      "adalora_tinit=0,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.95,\n",
      "adam_epsilon=1e-08,\n",
      "adapter_act=gelu,\n",
      "adapter_length=128,\n",
      "adapters=[],\n",
      "add_version=True,\n",
      "agent_template=None,\n",
      "attn_impl=None,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "bnb_4bit_compute_dtype=torch.bfloat16,\n",
      "bnb_4bit_quant_storage=None,\n",
      "bnb_4bit_quant_type=nf4,\n",
      "bnb_4bit_use_double_quant=True,\n",
      "boft_block_num=0,\n",
      "boft_block_size=4,\n",
      "boft_dropout=0.0,\n",
      "boft_n_butterfly_factor=1,\n",
      "check_model=True,\n",
      "ckpt_dir=None,\n",
      "columns={},\n",
      "create_checkpoint_symlink=False,\n",
      "custom_dataset_info=[],\n",
      "custom_register_path=[],\n",
      "data_seed=42,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=4,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "dataset=['./data/train.jsonl'],\n",
      "dataset_num_proc=1,\n",
      "dataset_shuffle=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=None,\n",
      "deepspeed={'fp16': {'enabled': 'auto', 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': 'auto'}, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'none', 'pin_memory': True}, 'allgather_partitions': True, 'allgather_bucket_size': 200000000.0, 'overlap_comm': False, 'reduce_scatter': True, 'reduce_bucket_size': 200000000.0, 'contiguous_gradients': True}, 'gradient_accumulation_steps': 'auto', 'gradient_clipping': 'auto', 'steps_per_print': 2000, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False},\n",
      "device_map=None,\n",
      "disable_tqdm=None,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "download_mode=reuse_dataset_if_exists,\n",
      "enable_cache=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_datasets=[],\n",
      "eval_datasets_args=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_generation_config=None,\n",
      "eval_limit=None,\n",
      "eval_on_start=False,\n",
      "eval_steps=50.0,\n",
      "eval_strategy=steps,\n",
      "eval_use_evalscope=False,\n",
      "eval_use_gather_object=False,\n",
      "external_plugins=[],\n",
      "fourier_n_frequency=2000,\n",
      "fourier_scaling=300.0,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "freeze_aligner=True,\n",
      "freeze_llm=False,\n",
      "freeze_parameters=[],\n",
      "freeze_parameters_ratio=0.0,\n",
      "freeze_vit=True,\n",
      "fsdp=,\n",
      "fsdp_config=None,\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_num=1,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "galore_cos_threshold=0.4,\n",
      "galore_gamma_proj=2,\n",
      "galore_optim_per_parameter=False,\n",
      "galore_proj_bits=4,\n",
      "galore_proj_group_size=256,\n",
      "galore_proj_quant=False,\n",
      "galore_proj_type=std,\n",
      "galore_quantization=False,\n",
      "galore_queue_size=5,\n",
      "galore_rank=128,\n",
      "galore_scale=1.0,\n",
      "galore_target_modules=None,\n",
      "galore_update_proj_gap=50,\n",
      "galore_with_embedding=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hqq_axis=None,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_args_error=False,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "init_weights=True,\n",
      "interleave_prob=None,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "lazy_tokenize=False,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "lisa_activated_layers=0,\n",
      "lisa_step_interval=20,\n",
      "llamapro_num_groups=None,\n",
      "llamapro_num_new_blocks=4,\n",
      "load_args=False,\n",
      "load_best_model_at_end=False,\n",
      "load_data_args=False,\n",
      "load_dataset_config=None,\n",
      "local_rank=0,\n",
      "local_repo_path=None,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/data/nvme1/weights/Qwen3_sft_eval/v2-20250603-131403/runs,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=steps,\n",
      "logprobs=False,\n",
      "lora_alpha=32,\n",
      "lora_bias=none,\n",
      "lora_dropout=0.05,\n",
      "lora_dtype=None,\n",
      "lora_ga_batch_size=2,\n",
      "lora_ga_direction=ArB2r,\n",
      "lora_ga_iters=2,\n",
      "lora_ga_max_length=1024,\n",
      "lora_ga_scale=stable,\n",
      "lora_ga_stable_gamma=16,\n",
      "lora_modules=[],\n",
      "lora_rank=8,\n",
      "lorap_lr_ratio=None,\n",
      "loss_scale=default,\n",
      "loss_type=None,\n",
      "lr_scheduler_kwargs=None,\n",
      "lr_scheduler_type=cosine,\n",
      "max_grad_norm=1.0,\n",
      "max_length=2048,\n",
      "max_memory={},\n",
      "max_new_tokens=64,\n",
      "max_pixels=None,\n",
      "max_steps=-1,\n",
      "metric=None,\n",
      "metric_for_best_model=loss,\n",
      "metric_warmup_step=0,\n",
      "model=/data/nvme1/weights/Qwen3-0.6B,\n",
      "model_author=['swift'],\n",
      "model_kwargs={},\n",
      "model_name=['swift-qwen3'],\n",
      "model_revision=None,\n",
      "model_type=qwen3,\n",
      "modules_to_save=[],\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "norm_bbox=None,\n",
      "num_beams=1,\n",
      "num_labels=None,\n",
      "num_train_epochs=1.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "optimizer=None,\n",
      "output_dir=/data/nvme1/weights/Qwen3_sft_eval/v2-20250603-131403,\n",
      "overwrite_output_dir=False,\n",
      "packing=False,\n",
      "padding_side=right,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "problem_type=None,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "quant_bits=None,\n",
      "quant_method=None,\n",
      "ray_scope=last,\n",
      "reft_args=None,\n",
      "reft_intervention_type=LoreftIntervention,\n",
      "reft_layer_key=None,\n",
      "reft_layers=None,\n",
      "reft_rank=4,\n",
      "remove_unused_columns=True,\n",
      "repetition_penalty=None,\n",
      "report_to=['swanlab'],\n",
      "response_prefix=None,\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "resume_only_model=False,\n",
      "rope_scaling=None,\n",
      "run_name=None,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=50.0,\n",
      "save_strategy=steps,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "sequence_parallel_size=1,\n",
      "shuffle_buffer_size=1000,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "split_dataset_ratio=0.0,\n",
      "stop_words=[],\n",
      "stopping_strategy=first_exhausted,\n",
      "stream=False,\n",
      "streaming=False,\n",
      "strict=False,\n",
      "swanlab_exp_name=/data/nvme1/weights/Qwen3_sft_eval/v2-20250603-131403,\n",
      "swanlab_mode=cloud,\n",
      "swanlab_project=swift-qwen3,\n",
      "swanlab_token=<SWANLAB_TOKEN>,\n",
      "swanlab_workspace=None,\n",
      "system=None,\n",
      "target_modules=['all-linear'],\n",
      "target_regex=None,\n",
      "task_type=causal_lm,\n",
      "temperature=0.0,\n",
      "template=qwen3,\n",
      "template_backend=swift,\n",
      "tf32=None,\n",
      "top_k=None,\n",
      "top_logprobs=None,\n",
      "top_p=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_dtype=torch.bfloat16,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "train_dataloader_shuffle=True,\n",
      "train_type=lora,\n",
      "trainable_parameters=[],\n",
      "truncation_strategy=delete,\n",
      "tuner_backend=peft,\n",
      "use_chat_template=True,\n",
      "use_cpu=False,\n",
      "use_dora=False,\n",
      "use_galore=False,\n",
      "use_hf=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "use_rslora=False,\n",
      "use_swift_lora=False,\n",
      "val_dataset=['./data/eval.jsonl'],\n",
      "val_dataset_shuffle=False,\n",
      "vera_d_initial=0.1,\n",
      "vera_dropout=0.0,\n",
      "vera_projection_prng_key=0,\n",
      "vera_rank=256,\n",
      "warmup_ratio=0.05,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.1,\n",
      "zero_hpz_partition_size=None,\n",
      ")\n",
      "[INFO:swift] Loading the model using model_dir: /data/nvme1/weights/Qwen3-0.6B\n",
      "[INFO:swift] model_kwargs: {'device_map': 'cuda:0'}\n",
      "[INFO:swift] model.hf_device_map: {'': device(type='cuda', index=0)}\n",
      "[INFO:swift] model_info: ModelInfo(model_type='qwen3', model_dir='/data/nvme1/weights/Qwen3-0.6B', torch_dtype=torch.bfloat16, max_model_len=40960, quant_method=None, quant_bits=None, rope_scaling=None, config=Qwen3Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 40960,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen3\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      ", task_type='causal_lm', num_labels=None)\n",
      "[INFO:swift] model.generation_config: GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"max_new_tokens\": 64,\n",
      "  \"pad_token_id\": 151643\n",
      "}\n",
      "\n",
      "[INFO:swift] default_system: None\n",
      "[INFO:swift] response_prefix: ''\n",
      "[INFO:swift] agent_template: hermes\n",
      "[INFO:swift] max_length: 2048\n",
      "[INFO:swift] norm_bbox: norm1000\n",
      "[INFO:swift] Start time of running main: 2025-06-03 13:14:06.573991\n",
      "[INFO:swift] create tmp_dir: /home/finetune/.cache/modelscope/hub/tmp/hf_datasets-zwr790yb\n",
      "Map: 100%|██████████████████████| 43896/43896 [00:00<00:00, 49636.43 examples/s]\n",
      "Map: 100%|██████████████████████| 43896/43896 [00:00<00:00, 51839.59 examples/s]\n",
      "Map: 100%|██████████████████████| 43896/43896 [00:00<00:00, 50380.20 examples/s]\n",
      "Map: 100%|██████████████████████| 43896/43896 [00:00<00:00, 49959.53 examples/s]\n",
      "Map: 100%|████████████████████████| 4922/4922 [00:00<00:00, 15615.88 examples/s]\n",
      "[INFO:swift] train_dataset: Dataset({\n",
      "    features: ['messages'],\n",
      "    num_rows: 43896\n",
      "})\n",
      "[INFO:swift] val_dataset: Dataset({\n",
      "    features: ['messages'],\n",
      "    num_rows: 4922\n",
      "})\n",
      "Map: 100%|████████████████████████| 4922/4922 [00:00<00:00, 15554.68 examples/s]\n",
      "[INFO:swift] The split dataset from the training set will be saved at: /data/nvme1/weights/Qwen3_sft_eval/v2-20250603-131403/val_dataset.jsonl.\n",
      "Map: 100%|████████████████████████| 4922/4922 [00:00<00:00, 15476.80 examples/s]\n",
      "Map: 100%|████████████████████████| 4922/4922 [00:00<00:00, 15714.54 examples/s]\n",
      "Map: 100%|███████████████████████| 43896/43896 [00:23<00:00, 1897.88 examples/s]\n",
      "Map: 100%|███████████████████████| 43896/43896 [00:23<00:00, 1889.65 examples/s]\n",
      "Map: 100%|███████████████████████| 43896/43896 [00:23<00:00, 1865.92 examples/s]\n",
      "Map: 100%|███████████████████████| 43896/43896 [00:24<00:00, 1810.41 examples/s]\n",
      "Map: 100%|█████████████████████████| 4922/4922 [00:02<00:00, 1926.91 examples/s]\n",
      "Map: 100%|█████████████████████████| 4922/4922 [00:02<00:00, 1918.57 examples/s]\n",
      "[INFO:swift] [INPUT_IDS] [151644, 8948, 198, 56568, 101909, 99878, 100623, 48692, 100168, 110498, 151645, 198, 151644, 872, 198, 101091, 5373, 111009, 5373, 101101, 5373, 99599, 1773, 151645, 198, 151644, 77091, 198, 43288, 100802, 99689, 71268, 100629, 101970, 109091, 3837, 77288, 104017, 74763, 104980, 78556, 3407, 101091, 9909, 95936, 7552, 14777, 99689, 102119, 111198, 57218, 99563, 101063, 9370, 99600, 3837, 29524, 108069, 100355, 100138, 5373, 102041, 33108, 100376, 101034, 106961, 99599, 33108, 99877, 104612, 3407, 111009, 9909, 54, 269, 5270, 7552, 104442, 32664, 106168, 106144, 5373, 104013, 57191, 104613, 9370, 108334, 33108, 102690, 101576, 1773, 111009, 73670, 102031, 105358, 100419, 3837, 29524, 114686, 57191, 113754, 101391, 3407, 101101, 9909, 32658, 7552, 100141, 63367, 100369, 57191, 102075, 101565, 57191, 99877, 9370, 100374, 3837, 102119, 100751, 100386, 57191, 101091, 100650, 1773, 101101, 102119, 106166, 101884, 100656, 100160, 57191, 100466, 3407, 99599, 9909, 1397, 7552, 111198, 46944, 67071, 99623, 101121, 9370, 116275, 5373, 99532, 33108, 107632, 100008, 107339, 101565, 3837, 100629, 114099, 62926, 102024, 71817, 99876, 100145, 1773, 46944, 99599, 104309, 67338, 41146, 101091, 100637, 36407, 39352, 41146, 101979, 101436, 90395, 106961, 99599, 100641, 104237, 57191, 36885, 12857, 101101, 3407, 100001, 99689, 101920, 47606, 106809, 33108, 72064, 1773, 77557, 3837, 46944, 99599, 106492, 100637, 87267, 102031, 99599, 57218, 105358, 99877, 104612, 3837, 105358, 111009, 87267, 99564, 101091, 101313, 33108, 100138, 1773, 99599, 108542, 106961, 99599, 101894, 104237, 3837, 100656, 17714, 101091, 100160, 68536, 101066, 1773, 151645]\n",
      "[INFO:swift] [INPUT] <|im_start|>system\n",
      "你是一个专业的人工智能助手<|im_end|>\n",
      "<|im_start|>user\n",
      "政治、崇拜、联合、国家。<|im_end|>\n",
      "<|im_start|>assistant\n",
      "这四个词都具有不同的含义，但它们也彼此相关。\n",
      "\n",
      "政治（Politics）一词通常指的是与政府有关的活动，如管理和实施政策、决策和法律以及与其他国家和组织的关系。\n",
      "\n",
      "崇拜（Worship）是指对某个信仰、人物或物品的忠诚和仰慕。崇拜可以涉及宗教实践，如祈祷或祭祀仪式。\n",
      "\n",
      "联合（Union）一般指两个或更多的实体或组织的结合，通常用于商业或政治领域。联合通常旨在实现共同目标或目的。\n",
      "\n",
      "国家（State）指的是一个由一定范围的领土、人民和统治机构组成的实体，具有主权并独立进行国际关系。一个国家可能会通过其政治制度来管理其内部事务，并与其他国家建立联盟或结成联合。\n",
      "\n",
      "这些词之间存在交叉和联系。例如，一个国家的政治制度可能涉及国家与宗教组织的关系，宗教崇拜可能影响政治观点和政策。国家也可能与其他国家形成联盟，共同为政治目标而努力。<|im_end|>\n",
      "[INFO:swift] [LABELS_IDS] [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 43288, 100802, 99689, 71268, 100629, 101970, 109091, 3837, 77288, 104017, 74763, 104980, 78556, 3407, 101091, 9909, 95936, 7552, 14777, 99689, 102119, 111198, 57218, 99563, 101063, 9370, 99600, 3837, 29524, 108069, 100355, 100138, 5373, 102041, 33108, 100376, 101034, 106961, 99599, 33108, 99877, 104612, 3407, 111009, 9909, 54, 269, 5270, 7552, 104442, 32664, 106168, 106144, 5373, 104013, 57191, 104613, 9370, 108334, 33108, 102690, 101576, 1773, 111009, 73670, 102031, 105358, 100419, 3837, 29524, 114686, 57191, 113754, 101391, 3407, 101101, 9909, 32658, 7552, 100141, 63367, 100369, 57191, 102075, 101565, 57191, 99877, 9370, 100374, 3837, 102119, 100751, 100386, 57191, 101091, 100650, 1773, 101101, 102119, 106166, 101884, 100656, 100160, 57191, 100466, 3407, 99599, 9909, 1397, 7552, 111198, 46944, 67071, 99623, 101121, 9370, 116275, 5373, 99532, 33108, 107632, 100008, 107339, 101565, 3837, 100629, 114099, 62926, 102024, 71817, 99876, 100145, 1773, 46944, 99599, 104309, 67338, 41146, 101091, 100637, 36407, 39352, 41146, 101979, 101436, 90395, 106961, 99599, 100641, 104237, 57191, 36885, 12857, 101101, 3407, 100001, 99689, 101920, 47606, 106809, 33108, 72064, 1773, 77557, 3837, 46944, 99599, 106492, 100637, 87267, 102031, 99599, 57218, 105358, 99877, 104612, 3837, 105358, 111009, 87267, 99564, 101091, 101313, 33108, 100138, 1773, 99599, 108542, 106961, 99599, 101894, 104237, 3837, 100656, 17714, 101091, 100160, 68536, 101066, 1773, 151645]\n",
      "[INFO:swift] [LABELS] [-100 * 28]这四个词都具有不同的含义，但它们也彼此相关。\n",
      "\n",
      "政治（Politics）一词通常指的是与政府有关的活动，如管理和实施政策、决策和法律以及与其他国家和组织的关系。\n",
      "\n",
      "崇拜（Worship）是指对某个信仰、人物或物品的忠诚和仰慕。崇拜可以涉及宗教实践，如祈祷或祭祀仪式。\n",
      "\n",
      "联合（Union）一般指两个或更多的实体或组织的结合，通常用于商业或政治领域。联合通常旨在实现共同目标或目的。\n",
      "\n",
      "国家（State）指的是一个由一定范围的领土、人民和统治机构组成的实体，具有主权并独立进行国际关系。一个国家可能会通过其政治制度来管理其内部事务，并与其他国家建立联盟或结成联合。\n",
      "\n",
      "这些词之间存在交叉和联系。例如，一个国家的政治制度可能涉及国家与宗教组织的关系，宗教崇拜可能影响政治观点和政策。国家也可能与其他国家形成联盟，共同为政治目标而努力。<|im_end|>\n",
      "Map: 100%|█████████████████████████| 4922/4922 [00:02<00:00, 1836.08 examples/s]\n",
      "Map: 100%|█████████████████████████| 4922/4922 [00:02<00:00, 1830.84 examples/s]\n",
      "Map: 100%|██████████████████████| 43896/43896 [00:04<00:00, 10732.67 examples/s]\n",
      "Map: 100%|██████████████████████| 43896/43896 [00:04<00:00, 10621.07 examples/s]\n",
      "Map:  80%|█████████████████▌    | 35000/43896 [00:03<00:00, 10524.93 examples/s][INFO:swift] Dataset Token Length: 161.205941±94.219086, min=25.000000, max=643.000000, size=43896\n",
      "Map: 100%|████████████████████████| 4922/4922 [00:00<00:00, 10664.38 examples/s]\n",
      "Map: 100%|████████████████████████| 4922/4922 [00:00<00:00, 10609.72 examples/s]\n",
      "[INFO:swift] Dataset Token Length: 160.600975±95.134282, min=28.000000, max=857.000000, size=4922\n",
      "[INFO:swift] The TrainArguments will be saved in: /data/nvme1/weights/Qwen3_sft_eval/v2-20250603-131403/args.json\n",
      "Map:  84%|██████████████████▌   | 37000/43896 [00:03<00:00, 10511.42 examples/s]/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/swift/trainers/mixin.py:86: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n",
      "Map:  93%|████████████████████▌ | 41000/43896 [00:03<00:00, 10517.23 examples/s]No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "[INFO:swift] lora_config: LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='/data/nvme1/weights/Qwen3-0.6B', revision=None, inference_mode=False, r=8, target_modules={'up_proj', 'gate_proj', 'down_proj', 'o_proj', 'q_proj', 'v_proj', 'k_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=[], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, lora_dtype=None, lorap_lr_ratio=None, lorap_emb_lr=1e-06)\n",
      "[INFO:swift] model: PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Qwen3ForCausalLM(\n",
      "      (model): Qwen3Model(\n",
      "        (embed_tokens): Embedding(151936, 1024)\n",
      "        (layers): ModuleList(\n",
      "          (0-27): 28 x Qwen3DecoderLayer(\n",
      "            (self_attn): Qwen3Attention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "            )\n",
      "            (mlp): Qwen3MLP(\n",
      "              (gate_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "            (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "          )\n",
      "        )\n",
      "        (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "        (rotary_emb): Qwen3RotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[INFO:swift] model_parameter_info: PeftModelForCausalLM: 601.0962M Params (5.0463M Trainable [0.8395%]), 0.0001M Buffers.\n",
      "/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/swift/trainers/mixin.py:86: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n",
      "Detected kernel version 5.4.119, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "[INFO:swift] The logging file will be saved in: /data/nvme1/weights/Qwen3_sft_eval/v2-20250603-131403/logging.jsonl\n",
      "Map: 100%|██████████████████████| 43896/43896 [00:04<00:00, 10492.56 examples/s]\n",
      "Map: 100%|██████████████████████| 43896/43896 [00:04<00:00, 10464.53 examples/s]\n",
      "Map: 100%|████████████████████████| 4922/4922 [00:00<00:00, 10527.53 examples/s]\n",
      "Map:  41%|█████████▊              | 2000/4922 [00:00<00:00, 10419.32 examples/s]/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/swift/trainers/mixin.py:86: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "Map: 100%|████████████████████████| 4922/4922 [00:00<00:00, 10389.10 examples/s]\n",
      "/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/swift/trainers/mixin.py:86: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Tracking run with swanlab version 0.6.0                                   \n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Run data will be saved locally in \u001b[35m\u001b[1m/home/finetune/workspace/lxy/evalscope_use_qwen/swanlog/run-20250603_131444-a3b1799d\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: 👋 Hi \u001b[1m\u001b[39mLiXinYu\u001b[0m\u001b[0m, welcome to swanlab!\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Syncing run \u001b[33m/data/nvme1/weights/Qwen3_sft_eval/v2-20250603-131403\u001b[0m to the cloud\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: 🏠 View project at \u001b[34m\u001b[4mhttps://swanlab.cn/@LiXinYu/swift-qwen3\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://swanlab.cn/@LiXinYu/swift-qwen3/runs/anj9rghykbt2r16p7lpsu\u001b[0m\u001b[0m\n",
      "{'loss': 2.2868824, 'token_acc': 0.55850234, 'grad_norm': 2.69819403, 'learning_rate': 7.2e-07, 'memory(GiB)': 2.29, 'train_speed(iter/s)': 0.155211, 'epoch': 0.0, 'global_step/max_steps': '1/2744', 'percentage': '0.04%', 'elapsed_time': '1s', 'remaining_time': '53m 7s'}\n",
      "{'loss': 2.20958591, 'token_acc': 0.57862462, 'grad_norm': 2.87938261, 'learning_rate': 3.62e-06, 'memory(GiB)': 3.35, 'train_speed(iter/s)': 0.511772, 'epoch': 0.0, 'global_step/max_steps': '5/2744', 'percentage': '0.18%', 'elapsed_time': '4s', 'remaining_time': '40m 59s'}\n",
      "{'loss': 2.36494598, 'token_acc': 0.51521061, 'grad_norm': 2.78143311, 'learning_rate': 7.25e-06, 'memory(GiB)': 3.35, 'train_speed(iter/s)': 0.722531, 'epoch': 0.0, 'global_step/max_steps': '10/2744', 'percentage': '0.36%', 'elapsed_time': '8s', 'remaining_time': '39m 0s'}\n",
      "{'loss': 2.29972343, 'token_acc': 0.52727938, 'grad_norm': 2.76850629, 'learning_rate': 1.087e-05, 'memory(GiB)': 3.35, 'train_speed(iter/s)': 0.838251, 'epoch': 0.01, 'global_step/max_steps': '15/2744', 'percentage': '0.55%', 'elapsed_time': '12s', 'remaining_time': '38m 14s'}\n",
      "{'loss': 2.30612793, 'token_acc': 0.51032258, 'grad_norm': 2.72370219, 'learning_rate': 1.449e-05, 'memory(GiB)': 3.35, 'train_speed(iter/s)': 0.914375, 'epoch': 0.01, 'global_step/max_steps': '20/2744', 'percentage': '0.73%', 'elapsed_time': '16s', 'remaining_time': '37m 39s'}\n",
      "{'loss': 2.38589554, 'token_acc': 0.53606028, 'grad_norm': 2.67608237, 'learning_rate': 1.812e-05, 'memory(GiB)': 3.35, 'train_speed(iter/s)': 0.96706, 'epoch': 0.01, 'global_step/max_steps': '25/2744', 'percentage': '0.91%', 'elapsed_time': '20s', 'remaining_time': '37m 17s'}\n",
      "{'loss': 2.14887009, 'token_acc': 0.5402205, 'grad_norm': 2.14527059, 'learning_rate': 2.174e-05, 'memory(GiB)': 3.35, 'train_speed(iter/s)': 1.005142, 'epoch': 0.01, 'global_step/max_steps': '30/2744', 'percentage': '1.09%', 'elapsed_time': '24s', 'remaining_time': '37m 2s'}\n",
      "{'loss': 2.04033852, 'token_acc': 0.56593236, 'grad_norm': 1.63854575, 'learning_rate': 2.536e-05, 'memory(GiB)': 3.35, 'train_speed(iter/s)': 1.034994, 'epoch': 0.01, 'global_step/max_steps': '35/2744', 'percentage': '1.28%', 'elapsed_time': '28s', 'remaining_time': '36m 48s'}\n",
      "{'loss': 2.10077934, 'token_acc': 0.52582948, 'grad_norm': 2.04897547, 'learning_rate': 2.899e-05, 'memory(GiB)': 3.35, 'train_speed(iter/s)': 1.057179, 'epoch': 0.01, 'global_step/max_steps': '40/2744', 'percentage': '1.46%', 'elapsed_time': '32s', 'remaining_time': '36m 40s'}\n",
      "{'loss': 2.05695229, 'token_acc': 0.54703062, 'grad_norm': 1.8697449, 'learning_rate': 3.261e-05, 'memory(GiB)': 3.35, 'train_speed(iter/s)': 1.076037, 'epoch': 0.02, 'global_step/max_steps': '45/2744', 'percentage': '1.64%', 'elapsed_time': '36s', 'remaining_time': '36m 31s'}\n",
      "{'loss': 2.05837421, 'token_acc': 0.60347376, 'grad_norm': 2.86571503, 'learning_rate': 3.623e-05, 'memory(GiB)': 3.35, 'train_speed(iter/s)': 1.091897, 'epoch': 0.02, 'global_step/max_steps': '50/2744', 'percentage': '1.82%', 'elapsed_time': '40s', 'remaining_time': '36m 22s'}\n",
      "Train:   2%|▌                                 | 50/2744 [00:40<35:40,  1.26it/s]\n",
      "Val: 100%|█████████████████████████████████▉| 1229/1231 [00:48<00:00, 26.61it/s]\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 50 on key train/global_step already exists, ignored.\n",
      "{'eval_loss': 2.11910152, 'eval_token_acc': 0.55387852, 'eval_runtime': 48.392, 'eval_samples_per_second': 101.711, 'eval_steps_per_second': 25.438, 'epoch': 0.02, 'global_step/max_steps': '50/2744', 'percentage': '1.82%', 'elapsed_time': '1m 28s', 'remaining_time': '1h 19m 50s'}\n",
      "Val: 100%|██████████████████████████████████| 1231/1231 [00:48<00:00, 25.53it/s]\n",
      "[INFO:swift] Saving model checkpoint to /data/nvme1/weights/Qwen3_sft_eval/v2-20250603-131403/checkpoint-50\n",
      "{'loss': 1.97732391, 'token_acc': 0.55702927, 'grad_norm': 2.10110831, 'learning_rate': 3.986e-05, 'memory(GiB)': 3.65, 'train_speed(iter/s)': 0.551535, 'epoch': 0.02, 'global_step/max_steps': '55/2744', 'percentage': '2.00%', 'elapsed_time': '1m 34s', 'remaining_time': '1h 16m 57s'}\n",
      "{'loss': 1.90263138, 'token_acc': 0.56265256, 'grad_norm': 1.52956069, 'learning_rate': 4.348e-05, 'memory(GiB)': 3.65, 'train_speed(iter/s)': 0.578934, 'epoch': 0.02, 'global_step/max_steps': '60/2744', 'percentage': '2.19%', 'elapsed_time': '1m 38s', 'remaining_time': '1h 13m 19s'}\n",
      "{'loss': 1.82190895, 'token_acc': 0.54651615, 'grad_norm': 1.32120311, 'learning_rate': 4.71e-05, 'memory(GiB)': 3.65, 'train_speed(iter/s)': 0.604345, 'epoch': 0.02, 'global_step/max_steps': '65/2744', 'percentage': '2.37%', 'elapsed_time': '1m 42s', 'remaining_time': '1h 10m 15s'}\n",
      "{'loss': 1.98300362, 'token_acc': 0.52293271, 'grad_norm': 2.54580569, 'learning_rate': 5.072e-05, 'memory(GiB)': 3.65, 'train_speed(iter/s)': 0.627995, 'epoch': 0.03, 'global_step/max_steps': '70/2744', 'percentage': '2.55%', 'elapsed_time': '1m 46s', 'remaining_time': '1h 7m 36s'}\n",
      "{'loss': 1.90081654, 'token_acc': 0.56767459, 'grad_norm': 1.64687693, 'learning_rate': 5.435e-05, 'memory(GiB)': 3.65, 'train_speed(iter/s)': 0.650127, 'epoch': 0.03, 'global_step/max_steps': '75/2744', 'percentage': '2.73%', 'elapsed_time': '1m 50s', 'remaining_time': '1h 5m 17s'}\n",
      "{'loss': 1.94524994, 'token_acc': 0.52907599, 'grad_norm': 1.32121086, 'learning_rate': 5.797e-05, 'memory(GiB)': 3.65, 'train_speed(iter/s)': 0.670882, 'epoch': 0.03, 'global_step/max_steps': '80/2744', 'percentage': '2.92%', 'elapsed_time': '1m 53s', 'remaining_time': '1h 3m 15s'}\n",
      "{'loss': 1.87360249, 'token_acc': 0.55203816, 'grad_norm': 1.27009058, 'learning_rate': 6.159e-05, 'memory(GiB)': 3.65, 'train_speed(iter/s)': 0.690216, 'epoch': 0.03, 'global_step/max_steps': '85/2744', 'percentage': '3.10%', 'elapsed_time': '1m 57s', 'remaining_time': '1h 1m 27s'}\n",
      "{'loss': 1.73058338, 'token_acc': 0.57807196, 'grad_norm': 1.54148996, 'learning_rate': 6.522e-05, 'memory(GiB)': 3.65, 'train_speed(iter/s)': 0.708477, 'epoch': 0.03, 'global_step/max_steps': '90/2744', 'percentage': '3.28%', 'elapsed_time': '2m 1s', 'remaining_time': '59m 50s'}\n",
      "{'loss': 1.87540207, 'token_acc': 0.58182376, 'grad_norm': 1.39831185, 'learning_rate': 6.884e-05, 'memory(GiB)': 3.65, 'train_speed(iter/s)': 0.725553, 'epoch': 0.03, 'global_step/max_steps': '95/2744', 'percentage': '3.46%', 'elapsed_time': '2m 5s', 'remaining_time': '58m 23s'}\n",
      "{'loss': 1.91621532, 'token_acc': 0.5565032, 'grad_norm': 1.81163073, 'learning_rate': 7.246e-05, 'memory(GiB)': 3.65, 'train_speed(iter/s)': 0.741599, 'epoch': 0.04, 'global_step/max_steps': '100/2744', 'percentage': '3.64%', 'elapsed_time': '2m 9s', 'remaining_time': '57m 5s'}\n",
      "Train:   4%|█▏                               | 100/2744 [02:09<34:30,  1.28it/s]\n",
      "Val: 100%|█████████████████████████████████▉| 1229/1231 [00:47<00:00, 27.16it/s]\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 100 on key train/epoch already exists, ignored.\n",
      "\u001b[1m\u001b[33mswanlab\u001b[0m\u001b[0m: Step 100 on key train/global_step already exists, ignored.\n",
      "{'eval_loss': 1.97419453, 'eval_token_acc': 0.56205969, 'eval_runtime': 47.6047, 'eval_samples_per_second': 103.393, 'eval_steps_per_second': 25.859, 'epoch': 0.04, 'global_step/max_steps': '100/2744', 'percentage': '3.64%', 'elapsed_time': '2m 57s', 'remaining_time': '1h 18m 4s'}\n",
      "Val: 100%|██████████████████████████████████| 1231/1231 [00:47<00:00, 25.95it/s]\n",
      "[INFO:swift] Saving model checkpoint to /data/nvme1/weights/Qwen3_sft_eval/v2-20250603-131403/checkpoint-100\n",
      "{'loss': 1.6258009, 'token_acc': 0.56461895, 'grad_norm': 1.87040734, 'learning_rate': 7.609e-05, 'memory(GiB)': 3.65, 'train_speed(iter/s)': 0.558853, 'epoch': 0.04, 'global_step/max_steps': '105/2744', 'percentage': '3.83%', 'elapsed_time': '3m 2s', 'remaining_time': '1h 16m 29s'}\n",
      "{'loss': 1.84048996, 'token_acc': 0.50386598, 'grad_norm': 1.44833982, 'learning_rate': 7.971e-05, 'memory(GiB)': 3.65, 'train_speed(iter/s)': 0.573606, 'epoch': 0.04, 'global_step/max_steps': '110/2744', 'percentage': '4.01%', 'elapsed_time': '3m 6s', 'remaining_time': '1h 14m 25s'}\n",
      "{'loss': 1.84601669, 'token_acc': 0.63735225, 'grad_norm': 1.59516668, 'learning_rate': 8.333e-05, 'memory(GiB)': 3.65, 'train_speed(iter/s)': 0.58779, 'epoch': 0.04, 'global_step/max_steps': '115/2744', 'percentage': '4.19%', 'elapsed_time': '3m 10s', 'remaining_time': '1h 12m 31s'}\n",
      "{'loss': 1.87282181, 'token_acc': 0.58772203, 'grad_norm': 1.71552062, 'learning_rate': 8.696e-05, 'memory(GiB)': 3.65, 'train_speed(iter/s)': 0.601349, 'epoch': 0.04, 'global_step/max_steps': '120/2744', 'percentage': '4.37%', 'elapsed_time': '3m 14s', 'remaining_time': '1h 10m 48s'}\n",
      "Train:   4%|█▍                               | 122/2744 [03:15<34:26,  1.27it/s]\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: KeyboardInterrupt by user\n",
      "W0603 13:18:01.113000 276345 site-packages/torch/distributed/elastic/agent/server/api.py:719] Received Signals.SIGINT death signal, shutting down workers\n",
      "W0603 13:18:01.114000 276345 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 276412 closing signal SIGINT\n",
      "Error in sys.excepthook:\n",
      "Process Process-2:\n",
      "Traceback (most recent call last):\n",
      "Process Process-4:\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/linecache.py\", line 46, in getlines\n",
      "Process Process-1:\n",
      "Process Process-3:\n",
      "W0603 13:18:01.114000 276345 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 276413 closing signal SIGINT\n",
      "W0603 13:18:01.114000 276345 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 276414 closing signal SIGINT\n",
      "W0603 13:18:01.114000 276345 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 276415 closing signal SIGINT\n",
      "    return updatecache(filename, module_globals)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/linecache.py\", line 137, in updatecache\n",
      "    lines = fp.readlines()\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/codecs.py\", line 319, in decode\n",
      "Process Process-4:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py\", line 317, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py\", line 317, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py\", line 317, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py\", line 317, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/multiprocessing/process.py\", line 317, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/shutil.py\", line 630, in _rmtree_safe_fd\n",
      "    entries = list(scandir_it)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finaliz^C\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1857, in _call_impl\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/subprocess.py\", line 1209, in wait\n",
      "    return self._wait(timeout=timeout)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/subprocess.py\", line 1959, in _wait\n",
      "    return inner()\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1805, in inner\n",
      "        result = forward_call(*args, **kwargs)(pid, sts) = self._try_wait(0)\n",
      "\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/subprocess.py\", line 1917, in _try_wait\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/peft/peft_model.py\", line 1757, in forward\n",
      "    return self.base_model(\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/finetune/miniconda3/envs/eval/bin/swift\", line 8, in <module>\n",
      "    sys.exit(cli_main())\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/swift/cli/main.py\", line 70, in cli_main\n",
      "    result = subprocess.run(args)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/subprocess.py\", line 505, in run\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "    stdout, stderr = process.communicate(input, timeout=timeout)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/subprocess.py\", line 1146, in communicate\n",
      "    self.wait()\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/subprocess.py\", line 1222, in wait\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/peft/tuners/tuners_utils.py\", line 193, in forward\n",
      "    return self.model.forward(*args, **kwargs)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/transformers/utils/generic.py\", line 969, in wrapper\n",
      "    self._wait(timeout=sigint_timeout)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/subprocess.py\", line 1953, in _wait\n",
      "    output = func(self, *args, **kwargs)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/transformers/models/qwen3/modeling_qwen3.py\", line 730, in forward\n",
      "    outputs: BaseModelOutputWithPast = self.model(    \n",
      "time.sleep(delay)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/transformers/utils/generic.py\", line 969, in wrapper\n",
      "    output = func(self, *args, **kwargs)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/transformers/models/qwen3/modeling_qwen3.py\", line 463, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/transformers/modeling_layers.py\", line 47, in __call__\n",
      "    return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/swift/trainers/arguments.py\", line 73, in _new_checkpoint\n",
      "    return _old_checkpoint(*args, use_reentrant=use_reentrant_, **kwargs)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/torch/_compile.py\", line 51, in inner\n",
      "    return disable_fn(*args, **kwargs)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 838, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 488, in checkpoint\n",
      "    return CheckpointFunction.apply(function, preserve, *args)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/torch/autograd/function.py\", line 575, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 263, in forward\n",
      "    outputs = run_function(*args)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/transformers/models/qwen3/modeling_qwen3.py\", line 284, in forward\n",
      "    hidden_states, self_attn_weights = self.self_attn(\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/transformers/models/qwen3/modeling_qwen3.py\", line 215, in forward\n",
      "    value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/peft/tuners/lora/layer.py\", line 727, in forward\n",
      "    result = result + lora_B(lora_A(dropout(x))) * scaling\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/swift/tuners/peft.py\", line 276, in keep_device_forward\n",
      "    return self.forward_origin(*args, **kwargs)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "KeyboardInterrupt\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Experiment \u001b[33m/data/nvme1/weights/Qwen3_sft_eval/v2-20250603-131403\u001b[0m has completed\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: 🏠 View project at \u001b[34m\u001b[4mhttps://swanlab.cn/@LiXinYu/swift-qwen3\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://swanlab.cn/@LiXinYu/swift-qwen3/runs/anj9rghykbt2r16p7lpsu\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: \\ Waiting for uploading completeException ignored in atexit callback: <function shutdown_compile_workers at 0x7f1054f791b0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/torch/_inductor/async_compile.py\", line 113, in shutdown_compile_workers\n",
      "    pool.shutdown()\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 239, in shutdown\n",
      "    self.process.wait(300)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/subprocess.py\", line 1209, in wait\n",
      "    return self._wait(timeout=timeout)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/subprocess.py\", line 1953, in _wait\n",
      "    time.sleep(delay)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py\", line 73, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 279558) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.\n",
      "[rank1]: Traceback (most recent call last):\n",
      "[rank1]:   File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/swift/cli/sft.py\", line 7, in <module>\n",
      "[rank1]:     sft_main()\n",
      "[rank1]:   File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/swift/llm/train/sft.py\", line 281, in sft_main\n",
      "[rank1]:     return SwiftSft(args).main()\n",
      "[rank1]:   File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/swift/llm/base.py\", line 47, in main\n",
      "[rank1]:     result = self.run()\n",
      "[rank1]:   File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/swift/llm/train/sft.py\", line 147, in run\n",
      "[rank1]:     return self.train(trainer)\n",
      "[rank1]:   File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/swift/llm/train/sft.py\", line 207, in train\n",
      "[rank1]:     trainer.train(trainer.args.resume_from_checkpoint)\n",
      "[rank1]:   File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/swift/trainers/mixin.py\", line 321, in train\n",
      "[rank1]:     res = super().train(*args, **kwargs)\n",
      "[rank1]:   File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/transformers/trainer.py\", line 2240, in train\n",
      "[rank1]:     return inner_training_loop(\n",
      "[rank1]:   File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/transformers/trainer.py\", line 2555, in _inner_training_loop\n",
      "[rank1]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "[rank1]:   File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/transformers/trainer.py\", line 3791, in training_step\n",
      "[rank1]:     self.accelerator.backward(loss, **kwargs)\n",
      "[rank1]:   File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/accelerate/accelerator.py\", line 2465, in backward\n",
      "[rank1]:     self.deepspeed_engine_wrapped.backward(loss, **kwargs)\n",
      "[rank1]:   File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/accelerate/utils/deepspeed.py\", line 266, in backward\n",
      "[rank1]:     self.engine.backward(loss, **kwargs)\n",
      "[rank1]:   File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 20, in wrapped_fn\n",
      "[rank1]:     ret_val = func(*args, **kwargs)\n",
      "[rank1]:   File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 2225, in backward\n",
      "[rank1]:     self._do_optimizer_backward(loss, retain_graph)\n",
      "[rank1]:   File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 2171, in _do_optimizer_backward\n",
      "[rank1]:     self.optimizer.backward(loss, retain_graph=retain_graph)\n",
      "[rank1]:   File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py\", line 2099, in backward\n",
      "[rank1]:     self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)\n",
      "[rank1]:   File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py\", line 63, in backward\n",
      "[rank1]:     scaled_loss.backward(retain_graph=retain_graph)\n",
      "[rank1]:   File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/torch/_tensor.py\", line 648, in backward\n",
      "[rank1]:     torch.autograd.backward(\n",
      "[rank1]:   File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 353, in backward\n",
      "[rank1]:     _engine_run_backward(\n",
      "[rank1]:   File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/torch/autograd/graph.py\", line 824, in _engine_run_backward\n",
      "[rank1]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "[rank1]: KeyboardInterrupt\n",
      "[rank1]:[W603 13:18:01.234359254 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=81, addr=[VM-0-2-centos]:50220, remote=[VM-0-2-centos]:29500): failed to recv, got 0 bytes\n",
      "Exception raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:678 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7ff0e3a505e8 in /home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)\n",
      "frame #1: <unknown function> + 0x5ba8afe (0x7ff127b73afe in /home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: <unknown function> + 0x5baae40 (0x7ff127b75e40 in /home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: <unknown function> + 0x5bab74a (0x7ff127b7674a in /home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x2a9 (0x7ff127b701a9 in /home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7ff0e4d699a9 in /home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #6: <unknown function> + 0xd8198 (0x7ff0d4d56198 in /home/finetune/miniconda3/envs/eval/bin/../lib/libstdc++.so.6)\n",
      "frame #7: <unknown function> + 0x81ca (0x7ff14cdd31ca in /usr/lib64/libpthread.so.0)\n",
      "frame #8: clone + 0x43 (0x7ff14c2a48d3 in /usr/lib64/libc.so.6)\n",
      "\n",
      "[rank1]:[W603 13:18:01.236862729 ProcessGroupNCCL.cpp:1659] [PG ID 0 PG GUID 0(default_pg) Rank 1] Failed to check the \"should dump\" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: failed to recv, got 0 bytes\n",
      "[rank3]:[W603 13:18:01.236823510 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=81, addr=[VM-0-2-centos]:50222, remote=[VM-0-2-centos]:29500): failed to recv, got 0 bytes\n",
      "Exception raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:678 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7f13238115e8 in /home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libc10.so)\n",
      "frame #1: <unknown function> + 0x5ba8afe (0x7f1367934afe in /home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: <unknown function> + 0x5baae40 (0x7f1367936e40 in /home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: <unknown function> + 0x5bab74a (0x7f136793774a in /home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x2a9 (0x7f13679311a9 in /home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7f1324b2a9a9 in /home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #6: <unknown function> + 0xd8198 (0x7f1314b17198 in /home/finetune/miniconda3/envs/eval/bin/../lib/libstdc++.so.6)\n",
      "frame #7: <unknown function> + 0x81ca (0x7f138cb941ca in /usr/lib64/libpthread.so.0)\n",
      "frame #8: clone + 0x43 (0x7f138c0658d3 in /usr/lib64/libc.so.6)\n",
      "\n",
      "[rank3]:[W603 13:18:01.239272956 ProcessGroupNCCL.cpp:1659] [PG ID 0 PG GUID 0(default_pg) Rank 3] Failed to check the \"should dump\" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: failed to recv, got 0 bytes\n"
     ]
    }
   ],
   "source": [
    "!bash ms-swift-train/train_deepspeed.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0f36084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run sh: `/home/finetune/miniconda3/envs/eval/bin/python3.10 /home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/swift/cli/export.py --adapters /data/nvme1/weights/Qwen3_sft_eval/v5-20250603-151411/checkpoint-604 --model /data/nvme1/weights/Qwen3-0.6B --output_dir /data/nvme1/weights/Qwen3_sft_eval/output --merge_lora true`\n",
      "[INFO:swift] Successfully registered `/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/swift/llm/dataset/data/dataset_info.json`.\n",
      "[INFO:swift] Loading the model using model_dir: /data/nvme1/weights/Qwen3_sft_eval/v5-20250603-151411/checkpoint-604\n",
      "[INFO:swift] Successfully loaded /data/nvme1/weights/Qwen3_sft_eval/v5-20250603-151411/checkpoint-604/args.json.\n",
      "[INFO:swift] rank: -1, local_rank: -1, world_size: 1, local_world_size: 1\n",
      "[INFO:swift] Loading the model using model_dir: /data/nvme1/weights/Qwen3-0.6B\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/swift/cli/export.py\", line 5, in <module>\n",
      "    export_main()\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/swift/llm/export/export.py\", line 50, in export_main\n",
      "    return SwiftExport(args).main()\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/swift/llm/base.py\", line 18, in __init__\n",
      "    self.args = self._parse_args(args)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/swift/llm/base.py\", line 30, in _parse_args\n",
      "    args, remaining_argv = parse_args(self.args_class, args)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/swift/utils/utils.py\", line 151, in parse_args\n",
      "    args, remaining_args = parser.parse_args_into_dataclasses(argv, return_remaining_strings=True)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/transformers/hf_argparser.py\", line 358, in parse_args_into_dataclasses\n",
      "    obj = dtype(**inputs)\n",
      "  File \"<string>\", line 99, in __init__\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/swift/llm/argument/export_args.py\", line 105, in __post_init__\n",
      "    self._init_output_dir()\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/swift/llm/argument/export_args.py\", line 86, in _init_output_dir\n",
      "    raise FileExistsError(f'args.output_dir: `{self.output_dir}` already exists.')\n",
      "FileExistsError: args.output_dir: `/data/nvme1/weights/Qwen3_sft_eval/output` already exists.\n"
     ]
    }
   ],
   "source": [
    "!bash evalscope-eval/merge.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6775ab6",
   "metadata": {},
   "source": [
    "# 评估代码\n",
    "\n",
    "1. 基本使用\n",
    "2. 模型API服务评测\n",
    "3. 模型推理性能压测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723ddcb4",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. 基本使用\n",
    "\n",
    "因为本地磁盘空间不够，因此我会将所有的模型或者数据集存放到别处，如果本地磁盘空间足够，可以直接运行下面的代码查看\n",
    "\n",
    "**需要注意Qwen3可以开启或者关闭思考模式，因此需要额外添加该参数的设置：**\n",
    "\n",
    "--generation-config '{\"max_new_tokens\":2048,\"chat_template_kwargs\":{\"enable_thinking\": false}}' \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2032cf34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-04 12:48:59,763 - evalscope - INFO - Args: Task config is provided with CommandLine type.\n",
      "2025-06-04 12:49:01,582 - evalscope - INFO - Loading model /data/nvme1/weights/Qwen3_sft_eval/output ...\n",
      "ANTLR runtime and generated code versions disagree: 4.9.3!=4.7.2\n",
      "ANTLR runtime and generated code versions disagree: 4.9.3!=4.7.2\n",
      "ANTLR runtime and generated code versions disagree: 4.9.3!=4.7.2\n",
      "ANTLR runtime and generated code versions disagree: 4.9.3!=4.7.2\n",
      "2025-06-04 12:49:03,132 - evalscope - WARNING - Got local model dir: /data/nvme1/weights/Qwen3_sft_eval/output\n",
      "2025-06-04 12:49:03,132 - evalscope - INFO - Updating generation config ...\n",
      "2025-06-04 12:49:03,230 - evalscope - INFO - Dump task config to ./outputs/20250604_124859/configs/task_config_e86197.yaml\n",
      "2025-06-04 12:49:03,232 - evalscope - INFO - {\n",
      "    \"model\": \"/data/nvme1/weights/Qwen3_sft_eval/output\",\n",
      "    \"model_id\": \"output\",\n",
      "    \"model_args\": {\n",
      "        \"revision\": \"master\",\n",
      "        \"precision\": \"torch.float16\"\n",
      "    },\n",
      "    \"model_task\": \"text_generation\",\n",
      "    \"template_type\": null,\n",
      "    \"chat_template\": null,\n",
      "    \"datasets\": [\n",
      "        \"gsm8k\"\n",
      "    ],\n",
      "    \"dataset_args\": {\n",
      "        \"gsm8k\": {\n",
      "            \"name\": \"gsm8k\",\n",
      "            \"dataset_id\": \"modelscope/gsm8k\",\n",
      "            \"model_adapter\": \"generation\",\n",
      "            \"output_types\": [\n",
      "                \"generation\"\n",
      "            ],\n",
      "            \"subset_list\": [\n",
      "                \"main\"\n",
      "            ],\n",
      "            \"metric_list\": [\n",
      "                \"AverageAccuracy\"\n",
      "            ],\n",
      "            \"few_shot_num\": 4,\n",
      "            \"few_shot_random\": false,\n",
      "            \"train_split\": null,\n",
      "            \"eval_split\": \"test\",\n",
      "            \"prompt_template\": \"Question: {query}\\nLet's think step by step\\nAnswer:\",\n",
      "            \"system_prompt\": null,\n",
      "            \"query_template\": null,\n",
      "            \"pretty_name\": \"GSM8K\",\n",
      "            \"filters\": null,\n",
      "            \"extra_params\": {}\n",
      "        }\n",
      "    },\n",
      "    \"dataset_dir\": \"/home/finetune/.cache/modelscope/datasets\",\n",
      "    \"dataset_hub\": \"modelscope\",\n",
      "    \"generation_config\": {\n",
      "        \"max_new_tokens\": 2048,\n",
      "        \"chat_template_kwargs\": {\n",
      "            \"enable_thinking\": false\n",
      "        }\n",
      "    },\n",
      "    \"eval_type\": \"checkpoint\",\n",
      "    \"eval_backend\": \"Native\",\n",
      "    \"eval_config\": null,\n",
      "    \"stage\": \"all\",\n",
      "    \"limit\": 5,\n",
      "    \"eval_batch_size\": 1,\n",
      "    \"mem_cache\": false,\n",
      "    \"use_cache\": null,\n",
      "    \"work_dir\": \"./outputs/20250604_124859\",\n",
      "    \"outputs\": null,\n",
      "    \"ignore_errors\": false,\n",
      "    \"debug\": false,\n",
      "    \"dry_run\": false,\n",
      "    \"seed\": 42,\n",
      "    \"api_url\": null,\n",
      "    \"api_key\": \"EMPTY\",\n",
      "    \"timeout\": null,\n",
      "    \"stream\": false,\n",
      "    \"judge_strategy\": \"auto\",\n",
      "    \"judge_worker_num\": 1,\n",
      "    \"judge_model_args\": {}\n",
      "}\n",
      "2025-06-04 12:49:03,232 - evalscope - INFO - **** Start evaluating on dataset modelscope/gsm8k ****\n",
      "2025-06-04 12:49:03,232 - evalscope - INFO - Loading dataset from hub: modelscope/gsm8k\n",
      "2025-06-04 12:49:03,350 - evalscope - INFO - Loading dataset: dataset_name: modelscope/gsm8k > subsets: ['main']\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/finetune/miniconda3/envs/eval/bin/evalscope\", line 8, in <module>\n",
      "    sys.exit(run_cmd())\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/evalscope/cli/cli.py\", line 25, in run_cmd\n",
      "    cmd.execute()\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/evalscope/cli/start_eval.py\", line 32, in execute\n",
      "    run_task(self.args)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/evalscope/run.py\", line 31, in run_task\n",
      "    return run_single_task(task_cfg, run_time)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/evalscope/run.py\", line 44, in run_single_task\n",
      "    result = evaluate_model(task_cfg, outputs)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/evalscope/run.py\", line 126, in evaluate_model\n",
      "    res_dict = evaluator.eval()\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/evalscope/evaluator/evaluator.py\", line 440, in eval\n",
      "    prompts = self.load_dataset()\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/evalscope/evaluator/evaluator.py\", line 73, in load_dataset\n",
      "    dataset = self.data_adapter.load(\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/evalscope/benchmarks/data_adapter.py\", line 104, in load\n",
      "    data_dict = self.load_from_hub(\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/evalscope/benchmarks/data_adapter.py\", line 140, in load_from_hub\n",
      "    dataset = MsDataset.load(\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/modelscope/msdatasets/ms_dataset.py\", line 296, in load\n",
      "    with load_dataset_with_ctx(\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/contextlib.py\", line 135, in __enter__\n",
      "    return next(self.gen)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/modelscope/msdatasets/utils/hf_datasets_util.py\", line 1408, in load_dataset_with_ctx\n",
      "    dataset_res = DatasetsWrapperHF.load_dataset(*args, **kwargs)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/modelscope/msdatasets/utils/hf_datasets_util.py\", line 938, in load_dataset\n",
      "    builder_instance = DatasetsWrapperHF.load_dataset_builder(\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/modelscope/msdatasets/utils/hf_datasets_util.py\", line 1064, in load_dataset_builder\n",
      "    dataset_module = DatasetsWrapperHF.dataset_module_factory(\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/modelscope/msdatasets/utils/hf_datasets_util.py\", line 1251, in dataset_module_factory\n",
      "    dataset_readme_path = _download_repo_file(\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/modelscope/msdatasets/utils/hf_datasets_util.py\", line 314, in _download_repo_file\n",
      "    repo_file_path = cached_path(\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/datasets/utils/file_utils.py\", line 205, in cached_path\n",
      "    output_path = get_from_cache(\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/modelscope/msdatasets/utils/hf_file_utils.py\", line 223, in get_from_cache_ms\n",
      "    response = http_head_ms(\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/modelscope/msdatasets/utils/hf_file_utils.py\", line 95, in http_head_ms\n",
      "    response = _request_with_retry_ms(\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/modelscope/msdatasets/utils/hf_file_utils.py\", line 78, in _request_with_retry_ms\n",
      "    response = requests.request(method=method.upper(), url=url, timeout=timeout, **params)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/requests/api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/requests/adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 464, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 1093, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/urllib3/connection.py\", line 704, in connect\n",
      "    self.sock = sock = self._new_conn()\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/urllib3/connection.py\", line 198, in _new_conn\n",
      "    sock = connection.create_connection(\n",
      "  File \"/home/finetune/miniconda3/envs/eval/lib/python3.10/site-packages/urllib3/util/connection.py\", line 73, in create_connection\n",
      "    sock.connect(sa)\n",
      "KeyboardInterrupt\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!bash ./evalscope-eval/cli/easy.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d82878",
   "metadata": {},
   "source": [
    "如果本地磁盘空间不足，使用下面的命令行把数据集下载到本地磁盘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49af1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://modelscope.oss-cn-beijing.aliyuncs.com/open_data/benchmark/data.zip\n",
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d495c07",
   "metadata": {},
   "source": [
    "然后把dataset-args参数中local_path修改成自己的保存数据集地址，然后运行下面的代码。\n",
    "\n",
    "**需要注意的是，gsm8k的数据集评测时默认的prompt_template为Question: {query}\\nLet's think step by step\\nAnswer:。测试过后发现enable_thinking设置为false比true要高些，所以可以默认为false，但是其他的没有cot提示的测试集可以设置为true，尤其是数学推理的时候**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81760dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash evalscope-eval/cli/gsm8k.sh\n",
    "!bash evalscope-eval/cli/multi.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a37f471",
   "metadata": {},
   "source": [
    "---\n",
    "## 2、模型API服务评测\n",
    "\n",
    "这里适合于模型结构没有被evalscope集成或者有自定义评测标准使用，其实这一步也很简单，只需要下面三个步骤思路：\n",
    "\n",
    "1. **接收用户请求**：用户通过发送一个 POST 请求到 `http://127.0.0.1:25001/v1/chat/completions`，提交一个包含对话信息的 JSON 数据，例如用户的问题或指令。\n",
    "2. **处理请求并生成回复**：服务器接收到请求后，使用加载好的模型和分词器对用户输入的内容进行处理，通过生成函数生成模型的回答，并将回答格式化为规定的 OpenAI API 格式。\n",
    "3. **返回结果**：服务器将生成的回答以 JSON 格式返回给用户，用户可以通过客户端接收并查看模型生成的文本内容。\n",
    "\n",
    "先运行下面的代码连接服务端口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c79b64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./evalscope-eval/api_model/url.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20f6262",
   "metadata": {},
   "source": [
    "再开启一个新的terminal运行下面的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef30d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash ./evalscope-eval/api_model/eval_api_eval.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b83693d",
   "metadata": {},
   "source": [
    "最终结果如下\n",
    "\n",
    "![](./examples/notebook_result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12fbab9",
   "metadata": {},
   "source": [
    "---\n",
    "## 3、模型推理性能压测\n",
    "\n",
    "**评估模型在高负载下的表现**\n",
    "- 识别性能瓶颈：通过模拟高并发请求场景，可以发现模型在处理大量请求时可能出现的性能瓶颈，如响应时间过长、资源占用过高、吞吐量不足等问题。例如，当并发数逐渐增加时，观察模型的响应时间是否呈线性增长，从而判断模型在高负载下的处理能力。\n",
    "- 测试系统稳定性：确保模型在长时间、高负载运行下不会出现崩溃、异常中断或性能急剧下降的情况，从而评估模型服务的可靠性和稳定性，为模型在实际生产环境中的稳定运行提供保障。\n",
    "\n",
    "**优化模型性能**\n",
    "- 指导资源分配：根据压测结果，了解模型在不同并发级别下的资源需求，如 CPU、内存、显存等的使用情况，从而合理分配资源，提高资源利用率，避免资源浪费或不足。\n",
    "- 调整模型参数：发现模型在推理过程中的不足之处，如某些参数设置不合理导致性能不佳，进而对模型的超参数进行调整和优化，以提升模型的推理效率。\n",
    "\n",
    "**比较不同模型的性能**\n",
    "- 选择最优模型：在多个模型中选择最适合特定应用场景的模型。通过在相同的压测条件下对不同模型进行测试，比较它们的性能指标，如响应时间、吞吐量、资源消耗等，从而为实际应用选择性能最优的模型。\n",
    "- 评估模型改进效果：对于经过优化或改进的模型版本，通过与原始模型进行对比压测，验证改进措施是否有效提升了模型的推理性能，从而为模型的持续优化提供依据。\n",
    "为模型部署提供依据\n",
    "- 确定部署规模：根据模型在不同并发请求下的性能表现，预估实际应用中需要部署的模型实例数量、服务器规模等，以满足用户对模型服务的性能需求。\n",
    "制定应急预案：了解模型在极端情况下的表现，提前制定应对策略，如在流量高峰时的负载均衡策略、资源扩展方案等，以确保模型服务在各种情况下都能正常运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88587e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash evalscope-eval/swanlab/perf.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e76e1b",
   "metadata": {},
   "source": [
    "压测结果可以查看链接👉[SwanLab](https://swanlab.cn/@LiXinYu/perf_benchmark/runs/k0flil25zyxgt1097asz8/chart)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091f8453",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
